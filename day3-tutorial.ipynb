{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ee7ee1716c4c45807bbd53d0c1b373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Map(basemap={'url': 'https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png', 'max_zoom': 19, 'attrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from app import *\n",
    "%matplotlib inline\n",
    "\n",
    "app = JupyterSMV(in_features=\"sites/Sites_lf_geo.json\")\n",
    "app.ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# day 3 tutorial\n",
    "\n",
    "```\n",
    "pip install ipywidgets\n",
    "pip install ipyleaflet\n",
    "jupyter nbextension enable --py widgetsnbextension\n",
    "```\n",
    "**notes:**\n",
    "* old code that Yaxing might still want to use is at bottom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                            # core\n",
    "import json\n",
    "from io import StringIO              # for python2: import StringIO \n",
    "\n",
    "import requests                      # to download SMV data\n",
    "import numpy as np                   #\n",
    "import pandas as pd                  #\n",
    "import xarray as xr                  #\n",
    "from shapely.geometry import shape   #\n",
    "\n",
    "import ipywidgets as wg              # widgets and plotting\n",
    "import ipyleaflet as mwg \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm, colors\n",
    "from IPython.display import display\n",
    "\n",
    "auth = dict(ORNL_DAAC_USER_NUM=str(32863))             # Jack\n",
    "url = \"https://daac.ornl.gov/cgi-bin/viz/download.pl?\" # SMV\n",
    "hstyle = {\"color\": \"white\", \"fillOpacity\": 0.6}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMV Datasets\n",
    "\n",
    "\n",
    "[*docs/smvdatasets.csv*](docs/smvdatasets.csv) is a copy of the datasets table from the [SMV User Guide](https://daac.ornl.gov/soilmoisture/guide.html). Read it into a `pandas` data frame and display it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smvds = pd.read_csv(\"docs/smvdatasets.csv\", index_col=\"dataset\", header=0)\n",
    "smvds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example from file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"docs/daily-smap-ORNL-DAAC-PccIuo.txt\", header=4, index_col=\"time\")\n",
    "df.index = pd.to_datetime(df.index)        \n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[\"AirMOSS_L4_rootzone\"].str.split(\";\", n=2, expand=True)       # split pd column to 3\n",
    "data = data.replace('', np.nan)                                         # set '' to nan\n",
    "data = data.astype(float)                                               # set all to float\n",
    "data.columns = [\"AirMOSS_L4_rootzone_\"+s for s in [\"mean\",\"min\",\"max\"]] # set column names\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [14, 5]\n",
    "\n",
    "data[\"AirMOSS_L4_rootzone_mean\"].plot()\n",
    "data[\"AirMOSS_L4_rootzone_min\"].plot()\n",
    "data[\"AirMOSS_L4_rootzone_max\"].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------\n",
    "## Read USFS data from GeoJSON\n",
    "\n",
    "The original dataset was a shapefile, but we reprojected and saved as GeoJSON using *ogr2ogr* from the GDAL/OGR binaries package available at OSGeo.\n",
    "\n",
    "Let's open the GeoJSON and reorganize it as a pandas data frame. Read to a dictionary with `json.load` and print the first feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sites/Sites_lf_geo.json\", \"r\") as f:\n",
    "    shapes = json.load(f)\n",
    "\n",
    "features = shapes[\"features\"]\n",
    "\n",
    "feat = features[0]\n",
    "feat[\"properties\"][\"id\"] = 0\n",
    "feat[\"properties\"][\"style\"] = {\"weight\": 1, \"fillOpacity\": 0.5}\n",
    "\n",
    "prop = feat[\"properties\"]\n",
    "prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A feature's properties (AKA attributes) are stored in the \"properties\" element of the GeoJSON object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geom = feat[\"geometry\"]         # each feature has a geom\n",
    "geom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get mean, std from shapefile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pd.DataFrame({\n",
    "    \"mean\": [v for k,v in prop.items() if \"MEAN\" in k],\n",
    "    \"std\": [v for k,v in prop.items() if \"STD\" in k]})\n",
    "\n",
    "stats.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use shapely shape to see if inside:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgeom = shape(geom)     # Shapely.geometry.shape\n",
    "bnds = sgeom.bounds\n",
    "cent = sgeom.centroid\n",
    "\n",
    "sgeom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leaflet poly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmap = mwg.basemap_to_tiles(mwg.basemaps.Esri.WorldImagery)\n",
    "poly = mwg.GeoJSON(data=feat)\n",
    "points = mwg.LayerGroup()\n",
    "\n",
    "m1 = mwg.Map(\n",
    "    layers=(bmap, poly, points,), \n",
    "    center=(cent.y, cent.x), \n",
    "    zoom=9)\n",
    "\n",
    "m1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EASE Grid\n",
    "\n",
    "Spatial queries to the Soil Moisture Visualizer return data corresponding to 9- by 9-km cells within the EASE grid system. Read about the EASE grid at the NSIDC's web page: https://nsidc.org/data/ease\n",
    "\n",
    "The next two cells show how to select arrays of EASE grid sample points that fall within an input polygon so that they can be used to submit a series of data requests to the SMV. We will combine everything into one more function (**get_ease**) to use later in our batch processing routine.\n",
    "      \n",
    "**Two binary files contain the arrays corresponding to global EASE grid centroid latitudes and longitudes, respectively. Open the two files and read into `numpy` arrays:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lats = np.fromfile(\"docs/EASE2_M09km.lats.3856x1624x1.double\", dtype=np.float64).flatten() \n",
    "lons = np.fromfile(\"docs/EASE2_M09km.lons.3856x1624x1.double\", dtype=np.float64).flatten()\n",
    "crds = np.dstack((lats,lons))[0]\n",
    "crds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a 2-dimensional array of EASE grid centroids using some arbitrary latitude, longitude bounds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnds = sgeom.bounds \n",
    "bnds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the points inside the polygon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ease = crds[(bnds[1]<lats)&(lats<bnds[3])&(bnds[0]<lons)&(lons<bnds[2])]\n",
    "ease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make some points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in ease:\n",
    "    pt = mwg.CircleMarker(                       # map point\n",
    "        location=(p[0],p[1]),                    # lat,lon tuple\n",
    "        radius=7,                                # in pixels\n",
    "        stroke=False,\n",
    "        fill_opacity=0.6,\n",
    "        fill_color=\"black\")\n",
    "    points.add_layer(pt)\n",
    "    \n",
    "m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "points.clear_layers()\n",
    "point = lambda p: shape({\"coordinates\": p, \"type\": \"Point\"})\n",
    "\n",
    "for p in ease:\n",
    "    spt = point([p[1], p[0]])\n",
    "    if sgeom.contains(spt):\n",
    "        pt = mwg.CircleMarker(\n",
    "            location=(p[0],p[1]),\n",
    "            radius=7,\n",
    "            stroke=False,\n",
    "            fill_opacity=0.6,\n",
    "            fill_color=\"black\")\n",
    "        points.add_layer(pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a function that includes all of the logic for getting the list of EASE coordinates inside a polygon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ease(geom):\n",
    "    \"\"\" \"\"\"\n",
    "     \n",
    "    bnds = geom.bounds \n",
    "    ease = crds[(bnds[1]<lats)&(lats<bnds[3])&(bnds[0]<lons)&(lons<bnds[2])]\n",
    "    \n",
    "    pt = lambda p: shape({\"coordinates\": p, \"type\": \"Point\"})\n",
    "    inpoly = [[p[0],p[1]] for p in ease if geom.contains(pt([p[1], p[0]]))]\n",
    "    \n",
    "    return(inpoly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download a SMV dataset with `requests`\n",
    "Each request to SMV takes a latitude `&lt` and longitude `&ln`. This request is for (30,-100):       \n",
    "https://daac.ornl.gov/cgi-bin/viz/download.pl?lt=30&ln=-100&d=smap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt, ln = ease[0]\n",
    "url = \"https://daac.ornl.gov/cgi-bin/viz/download.pl?lt={lt}&ln={ln}&d=smap\".format(lt=lt,ln=ln)\n",
    "r = requests.get(url, cookies=dict(ORNL_DAAC_USER_NUM=\"10\"))\n",
    "f = StringIO(r.text)\n",
    "\n",
    "print(\"\\n\".join(f.readlines()[0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two functions **txt_to_pd** and **split_pd** do everything we've learned to this point: convert the request response to a text object, then a data frame; and parse the columns of strings into three new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_pd(response_text):\n",
    "    \"\"\"Parses response.text to data frame with date index.\"\"\"\n",
    "    \n",
    "    f = StringIO(response_text)                      # get file from string\n",
    "    df = pd.read_csv(f, header=4, index_col=\"time\")  # read to df\n",
    "    df.index = pd.to_datetime(df.index)              # convert index to dates\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "\n",
    "def split_pd(col):\n",
    "    \"\"\"Splits pd column by ; and set all values to float, nan.\"\"\"\n",
    "    \n",
    "    df = col.str.split(\";\",n=2,expand=True)           # split col by ;\n",
    "    df = df.replace('', np.nan)                       # set '' to nan\n",
    "    df = df.astype(float)                             # set all to float\n",
    "    df.columns = [\"mean\",\"min\",\"max\"]                 # add column names\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use these repeatedly to request an process the entire grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = txt_to_pd(r.text)                                # parse response.text to df\n",
    "dfs = {col: split_pd(df[col]) for col in df.columns}  # loop over cols and split to dfs\n",
    "\n",
    "dfs[\"SMAP_rootzone\"].tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat SMV data as a netCDF-like `xarray.Dataset`\n",
    "The function below converts SMV outputs to an `xarray.Dataset`. The structure provided by `xarray` is based on pandas, but is better suited (in my opinion) for organizing data that has a spatial component. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latatts = dict(\n",
    "    standard_name=\"latitude\",\n",
    "    long_name=\"sample latitude\",\n",
    "    units=\"degrees_north\")\n",
    "\n",
    "lonatts = dict(\n",
    "    standard_name=\"latitude\",\n",
    "    long_name=\"sample latitude\",\n",
    "    units=\"degrees_north\")\n",
    "\n",
    "s = xr.DataArray(data=[1], dims=[\"sample\"])\n",
    "latarr = xr.DataArray(data=[lt], coords=[s], dims=[\"sample\"], attrs=latatts)\n",
    "lonarr = xr.DataArray(data=[ln], coords=[s], dims=[\"sample\"], attrs=lonatts)\n",
    "\n",
    "latarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add one more step to the response -> pandas -> split pandas workflow by making an xarray dataset. Print the SMAP_rootzone dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd_to_xr(dataset, df):\n",
    "    \"\"\"Makes an xr.Dataset from a pandas column (series) and coords.\"\"\"\n",
    "    \n",
    "    a = smvds.loc[dataset].to_dict()\n",
    "    x = xr.DataArray(df, name=dataset, attrs=a)\n",
    "    x = x.rename(dict(dim_1=\"stat\"))\n",
    "    x.attrs[\"allnan\"] = int(np.isnan(np.nanmean(x.data)))\n",
    "    \n",
    "    return(x)\n",
    "\n",
    "\n",
    "ds = {c: pd_to_xr(c,d) for c,d in dfs.items()}\n",
    "xds = xr.merge(ds.values())\n",
    "xds = xds.assign_coords(lat=latarr, lon=lonarr)\n",
    "xds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is what a single SMV dataset looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xds[\"SMAP_surface\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases it may be advantageous to reorder the dimensions over which the data are arranged. You can transpose the 2-d array with [`xarray.Dataset.transpose`](http://xarray.pydata.org/en/stable/generated/xarray.Dataset.transpose.html):\n",
    "\n",
    "```\n",
    "xdsT = xds.transpose()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the \"plottable\" datasets\n",
    "Remember that we added an attribute to each SMV dataset that indicates whether or not the mean\\*min\\*max array is entirely nodata: *allnan*\n",
    "\n",
    "Exclude SMV datasets that are entirely nodata using [`xarray.Dataset.filter_by_attrs`](http://xarray.pydata.org/en/stable/generated/xarray.Dataset.filter_by_attrs.html#xarray.Dataset.filter_by_attrs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pds = xds.filter_by_attrs(allnan=0)\n",
    "pds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also filter by any of the other attribute(s) that we assigned from the SMV datasets table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pds.filter_by_attrs(source=\"SMAP\", soil_zone=\"rootzone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slice/filter using dimension-based criteria\n",
    "Filter by the *stat* dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pds.sel(stat=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or the *time* dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = pds.time.data\n",
    "print(time[10]); print(time[20])\n",
    "\n",
    "pds.sel(time=slice(time[10],time[20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature becomes more useful as you add more dimensions to your dataset. We'll use it to filter across three dimensions once we add more sample locations to this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pds[\"SMAP_surface\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a simple interactive plotting UI\n",
    "We use the logic above to drive the plotting UI. We can get a list of the attributes to filter by using list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = list(set([pds[d].attrs[\"source\"] for d in pds]))\n",
    "stype = list(set([pds[d].attrs[\"type\"] for d in pds]))\n",
    "soil_zone = list(set([pds[d].attrs[\"soil_zone\"] for d in pds]))\n",
    "\n",
    "print(source); print(stype); print(soil_zone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRACE has very few observations, so we don't really need the source filter. And all data are from spaceborne datasets, so the only relevant attribute filter for this dataset is the *soil_zone*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dates = pds.sel(stat=\"mean\").dropna(dim=\"time\", how=\"all\").time.data\n",
    "dates = dates.astype('M8[D]')\n",
    "\n",
    "time_slider = wg.SelectionRangeSlider(\n",
    "    options=dates, \n",
    "    index=(0, len(dates)-1),\n",
    "    continuous_update=False,\n",
    "    layout=wg.Layout(width=\"auto\"))\n",
    "\n",
    "widgets = dict(\n",
    "    Time=time_slider, \n",
    "    By=[\"None\", \"year\", \"month\", \"week\", \"day\"],\n",
    "    Zone=['surface', 'rootzone'],\n",
    "    Mean=True, Min=True, Max=True)\n",
    "\n",
    "# needs to run twice to switch from inline -->\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and display the plot ui:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.rcParams['figure.figsize'] = [12, 5]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "\n",
    "def update(Time, By, Zone, Mean, Min, Max):\n",
    "    \"\"\" \"\"\"\n",
    "\n",
    "    stat = [True]*3 if not any([Mean,Min,Max]) else [Mean,Min,Max]\n",
    "    data = pds.sel(stat=stat)                      # filter by stats\n",
    "    \n",
    "    data = data.filter_by_attrs(soil_zone=Zone)    # filter by attributes\n",
    "\n",
    "    data = data.sel(time=slice(Time[0],Time[1]))   # filter by time;\n",
    "    \n",
    "    xaxis = \"time\" if By == \"None\" else By         # new plot interval\n",
    "    if By is not \"None\":\n",
    "        data = data.sel(stat=\"mean\")\n",
    "        data = data.groupby(\"time.\"+str(By)).mean()\n",
    "\n",
    "    ax.clear()                                     # clear plot\n",
    "    for d in data:                                 # loop over vars\n",
    "        data[d].plot.line(x=xaxis, ax=ax)          # add line\n",
    "    fig.canvas.draw()                              # draw \n",
    "\n",
    "\n",
    "p = wg.interactive(update, **widgets);\n",
    "display(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organize a series of SMV samples into similar structure\n",
    "\n",
    "The capabilities of xarray aren't obvious until you add a second dimension to the data (excluding the unnecessary *stats* dim). You can do everything we just did with pandas. Let's look at our USFS polygon again with a new map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = mwg.LayerGroup()\n",
    "polys = mwg.LayerGroup(layers=(poly,))\n",
    "m2 = mwg.Map(layers=(polys, points, bmap), center=(cent.y, cent.x), zoom=9)\n",
    "\n",
    "m2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Container for point samples\n",
    "`Sample(<id>,<lat>,<lon>)`\n",
    "* id: an integer id unique to the sample within it's input polygon\n",
    "* lat, lon: latitude, longitude numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://daac.ornl.gov/cgi-bin/viz/download.pl?\"\n",
    "\n",
    "class Sample(object):\n",
    "\n",
    "    def __init__(self, i, lat, lon):\n",
    "        \"\"\"Inits with id,lat,lon; makes request string, map point.\"\"\"\n",
    "        self.id, self.lat, self.lon = i, lat, lon               # id, lat, lon\n",
    "        self.rurl = url+\"lt={0}&ln={1}&d=smap\".format(lt,ln)    # request url     \n",
    "        self.pt = mwg.CircleMarker(                             # map point\n",
    "            location=(lat,lon),                                 # lat,lon tuple\n",
    "            radius=7,                                           # in pixels\n",
    "            stroke=False,\n",
    "            fill_opacity=0.6,\n",
    "            fill_color=\"black\")\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        for arg, val in kwargs.items():\n",
    "            setattr(self.pt, arg, val)\n",
    "        \n",
    "    def submit(self):\n",
    "        \"\"\"Called by parent. Downloads url. Updates status.\"\"\"\n",
    "        self.response = requests.get(self.rurl, cookies=auth)   # submit SMV request\n",
    "        self.df = txt_to_pd(self.response.text)                 # read to pandas df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function we made before `get_ease` to get a list of EASE points inside the polygon, make a Sample for each, and organize inside a data frame. Print the first five rows of the data frame and display the updated map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "for i, pt in enumerate(get_ease(sgeom)):\n",
    "    s = Sample(i, pt[0], pt[1])                # make a Sample instance\n",
    "    points.add_layer(s.pt)                     # add map pt to points group\n",
    "    samples.append((i, pt[0], pt[1], s, None)) # append tuple to the list\n",
    "\n",
    "samples = pd.DataFrame(                        # convert list of tuples to df\n",
    "    samples, \n",
    "    columns=[\"id\", \"lat\", \"lon\", \"samp\", \"xr\"])\n",
    "\n",
    "print(samples.head(5)); m2                     # display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a couple more widgets purely for aesthetics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(samples.samp)\n",
    "progress = wg.IntProgress(value=0, min=0, max=n, description=\"Progress: \", layout=wg.Layout(width=\"95%\"))\n",
    "\n",
    "def submit_handler(b):\n",
    "    submit.disabled = True               # disable submit button\n",
    "    for samp in samples.samp:            # loop over sample pts\n",
    "        progress.value += 1              # update progress bar\n",
    "        samp.update(                     # update point style\n",
    "            stroke=True, \n",
    "            color=\"white\", \n",
    "            opacity=0.6)\n",
    "        samp.submit()                    # download the data\n",
    "\n",
    "submit = wg.Button(description='Submit', button_style='success')\n",
    "submit.on_click(submit_handler)\n",
    "\n",
    "wg.VBox([m2,wg.HBox([submit,progress])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully you didn't have any trouble downloading the data. Remember we made the class that binds a map marker to several other items including a pandas data frame that gets created when the sample is retrieved from the SMV.\n",
    "\n",
    "Check the data frame for sample zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples0 = samples.iloc[0]\n",
    "print(samples0); samples0.samp.df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks familiar. Use the steps that we learned before to convert to an xarray dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = xr.DataArray(data=[samples0.id], dims=[\"sample\"])\n",
    "y0 = xr.DataArray(data=[samples0.lat], coords=[s0], dims=[\"sample\"], attrs=latatts)\n",
    "x0 = xr.DataArray(data=[samples0.lon], coords=[s0], dims=[\"sample\"], attrs=lonatts)\n",
    "\n",
    "df0 = samples0.samp.df                                  # get the sample df\n",
    "dfs0 = {col: split_pd(df0[col]) for col in df0.columns} # loop over cols and split to dfs\n",
    "ds0 = {c: pd_to_xr(c,d) for c,d in dfs0.items()}        # make xr datasets for each smv\n",
    "xds0 = xr.merge(ds0.values())                           # merge to one xr dataset\n",
    "xds0 = xds0.assign_coords(lat=y0, lon=x0)               # add coordinate arrays sample \n",
    "xds0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we're at it, wrap all of that up in a function to apply to all of the samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_xr(samp):\n",
    "    \"\"\" \"\"\"\n",
    "    \n",
    "    # get sample, lat, lon xr arrays\n",
    "    s = xr.DataArray(data=[samp.id], dims=[\"sample\"])\n",
    "    y = xr.DataArray(data=[samp.lat], coords=[s], dims=[\"sample\"], attrs=latatts)\n",
    "    x = xr.DataArray(data=[samp.lon], coords=[s], dims=[\"sample\"], attrs=lonatts)\n",
    "\n",
    "    df = samp.df                                         # get the sample df\n",
    "    dfs = {col: split_pd(df[col]) for col in df.columns} # loop over cols and split to dfs\n",
    "    ds = {c: pd_to_xr(c,d) for c,d in dfs.items()}       # make xr datasets for each smv\n",
    "    xds = xr.merge(ds.values())                          # merge to one xr dataset\n",
    "    xds = xds.assign_coords(lat=y, lon=x)                # add coordinate arrays\n",
    "    \n",
    "    return(xds)\n",
    "    \n",
    "\n",
    "for ix, row in samples.iterrows():                       # loop over samples df\n",
    "    samples.at[ix, \"xr\"] = get_sample_xr(row.samp)       # add xr dataset to col\n",
    "    \n",
    "samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check sample ten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.iloc[10].xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, if we organized the data correctly, we can concatenate along the sample dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xds0 = xr.concat(samples.xr.tolist(), \"sample\")\n",
    "xds0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save as a netCDF. The dataset needs another variable and some special attributes to comply with CF (explain CF):\n",
    "* a variable that describes the sequence that makes up the sample dimension\n",
    "* a dataset-level attribute that inidcates the dataset's *featureType*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over sample dim sequence and make strings like: \"sample##\"\n",
    "sample_name_data = [\"sample\"+(\"%02d\" % s) for s in xds0.sample.data]\n",
    "\n",
    "xds0[\"sample_name\"] = xr.DataArray(        # make an xr array\n",
    "    data=sample_name_data, \n",
    "    dims=[\"sample\"], \n",
    "    attrs=dict(                            # cf attributes\n",
    "        long_name=\"sample name\", \n",
    "        cf_role=\"timeseries_id\"))\n",
    "\n",
    "xds0.attrs.update({\n",
    "    \"convention\": \"CF-1.6\", \n",
    "    \"featureType\": \"timeSeries\",\n",
    "    \"source\": \"Soil Moisture Visualizer\",\n",
    "    \"institution\": \"Oak Ridge National Laboratory Distributed Active Archive Center\"})\n",
    "\n",
    "xds0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save with [`xarray.Dataset.to_netcdf`](http://xarray.pydata.org/en/stable/generated/xarray.Dataset.to_netcdf.html#xarray.Dataset.to_netcdf):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xds0.to_netcdf(\"23samples.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extend the plotting UI\n",
    "We probably have more options for filters with a dataset this size. Let's see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpds = xds0.filter_by_attrs(allnan=0)\n",
    "xpds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## better colors\n",
    "1. use [`numpy.linspace`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linspace.html) to make an array of evenly-spaced values between 0-1 \n",
    "2. map values to **Set3** in [`matplotlib.cm`](https://matplotlib.org/api/cm_api.html) | [colormap reference](https://matplotlib.org/gallery/color/colormap_reference.html)\n",
    "3. convert to hexadecimal with [`matplotlib.colors.to_hex`](https://matplotlib.org/api/_as_gen/matplotlib.colors.to_hex.html#matplotlib.colors.to_hex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cspace = np.linspace(0.0, 1.0, len(features)) # 1\n",
    "rgb = cm.Set3(cspace)                         # 2\n",
    "cols = [colors.to_hex(c[0:3]) for c in rgb]   # 3\n",
    "\n",
    "cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAybe give them the option to import shapefile?\n",
    "```\n",
    "with open(\"sites/Sites_lf_geo.json\", \"r\") as f:\n",
    "    shapes = json.load(f)\n",
    "features = shapes[\"features\"]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1,m2,s0,df0,dfs0,xds0,samples = None,None,None,None,None,None,None\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "site_details = \"\"\"\n",
    "{FORESTNAME} ({FORESTNUMB})\n",
    "{DISTRICTNA} ({DISTRICTNU})\n",
    "REGION:   {REGION}\n",
    "ACRES:    {GIS_ACRES}\n",
    "MIN:      {MIN}\n",
    "MEDIAN:   {MEDIAN}\n",
    "MAX:      {MAX}\n",
    "RANGE:    {RANGE}\n",
    "SUM:      {SUM}\n",
    "VARIETY:  {VARIETY}\n",
    "MINORITY: {MINORITY}\n",
    "MAJORITY: {MAJORITY}\n",
    "COUNT:    {COUNT}\n",
    "\"\"\"\n",
    "\n",
    "out_style = dict(width=\"30%\", height=\"400px\", overflow_y=\"scroll\", overflow_x=\"hidden\", border=\"1px solid gray\")\n",
    "out = Output(layout=Layout(**out_style))\n",
    "\n",
    "out.clear_output()\n",
    "with out:\n",
    "    print(site_details.format(**lo.site))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import ipywidgets as wg\n",
    "import ipyleaflet as mwg\n",
    "from ipywidgets import Layout, Button, IntProgress, Output, HBox, VBox, HTML\n",
    "from ipyleaflet import Map, LayerGroup, GeoJSON, CircleMarker\n",
    "\n",
    "basemap = mwg.basemap_to_tiles(mwg.basemaps.Esri.WorldImagery)\n",
    "polys = LayerGroup()\n",
    "points = LayerGroup()\n",
    "\n",
    "map_center = (32.75, -109)\n",
    "mapw = Map(\n",
    "    layers=(basemap, polys, points,), \n",
    "    center=map_center, \n",
    "    zoom=7, \n",
    "    scroll_wheel_zoom=True)\n",
    "\n",
    "submit = Button( \n",
    "    description='Submit', \n",
    "    disabled=True, \n",
    "    button_style='success')\n",
    "\n",
    "progress = IntProgress(\n",
    "    description=\"Progress: \", \n",
    "    layout=Layout(width=\"95%\"))\n",
    "\n",
    "ui = VBox([mapw, HBox([submit, progress])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample class, few small changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "url = \"https://daac.ornl.gov/cgi-bin/viz/download.pl?\"\n",
    "\n",
    "pt_style = dict(radius=7, fill_opacity=0.6, fill_color=\"black\", stroke=False)\n",
    "pt_status_on = dict(stroke=True, color=\"white\", opacity=0.6)\n",
    "pt_status_off = dict(stroke=False, color=\"black\", opacity=0.6)\n",
    "\n",
    "\n",
    "class Sample(object):\n",
    "\n",
    "    def __init__(self, i, lat, lon):\n",
    "        \"\"\"Inits with id,lat,lon; makes request string, map point.\"\"\"\n",
    "        self.id, self.lat, self.lon = i, lat, lon\n",
    "        self.rurl = url+\"lt={0}&ln={1}&d=smap\".format(lat,lon)  # request url\n",
    "        self.pt = CircleMarker(location=(lat, lon), **pt_style) # map point\n",
    "        self.on = False                                         # on/off status\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        for arg, val in kwargs.items():\n",
    "            setattr(self.pt, arg, val)\n",
    "    \n",
    "    def toggle(self, event, type, coordinates):\n",
    "        opac = 0.1 if self.on else 0.6\n",
    "        self.update(opacity=opac)\n",
    "        self.on = False if self.on else True\n",
    "        \n",
    "    def submit(self):\n",
    "        \"\"\"Called by parent. Downloads url. Updates status.\"\"\"\n",
    "        self.response = requests.get(self.rurl, cookies=auth)   # submit SMV request\n",
    "        self.df = txt_to_pd(self.response.text)                 # read to pandas df\n",
    "        self.xr = get_sample_xr(self)                           # get xarray dataset\n",
    "        self.pt.on_click(self.toggle)                           # callback on click\n",
    "        self.on = True                                          # toggle on\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "lyr_style = lambda c: {\"color\": c, \"fillColor\": c, \"weight\": 1, \"fillOpacity\": 0.4}\n",
    "lyr_hstyle = {\"color\": \"white\", \"fillOpacity\": 0.8}\n",
    "statcheck = lambda k: k.split(\"_\")[0] not in [\"MEAN\",\"STD\",\"Count\", \"style\"]\n",
    "\n",
    "\n",
    "def mgeo(i, feat, col):\n",
    "    feat[\"properties\"].update({\n",
    "        \"id\": i, \n",
    "        \"style\": lyr_style(col)})\n",
    "    return(dict(data=feat, hover_style=lyr_hstyle))\n",
    "\n",
    "\n",
    "class Layer(object):\n",
    "\n",
    "    def __init__(self, i, feat, col=None):\n",
    "        \"\"\"Inits with id,lat,lon; makes request string, map point.\"\"\"\n",
    "        self.id = i\n",
    "        self.feat = feat\n",
    "        \n",
    "        self.sgeom = shape(feat[\"geometry\"])\n",
    "        self.ease = get_ease(self.sgeom)                        # get ease points\n",
    "        self.cent = self.sgeom.centroid                         # get centroid\n",
    "        self.lat, self.lon = self.cent.y, self.cent.x           # get lat, lon\n",
    "        \n",
    "        prop = feat[\"properties\"]\n",
    "        mean = [v for k,v in prop.items() if \"MEAN\" in k]\n",
    "        std = [v for k,v in prop.items() if \"STD\" in k]\n",
    "        self.stats = pd.DataFrame({\"mean\": mean, \"std\": std})\n",
    "        self.site = {k:v for k,v in prop.items() if statcheck(k)}\n",
    "        \n",
    "        lyr = mgeo(i, feat, col)\n",
    "        self.layer = GeoJSON(**lyr)\n",
    "        self.layer.on_click(self.toggle)\n",
    "\n",
    "        self.on, self.dl = False, False                    # on/off, dl status\n",
    "        \n",
    "    def update(self, **kwargs):\n",
    "        for arg, val in kwargs.items():\n",
    "            setattr(self.layer, arg, val)\n",
    "    \n",
    "    def toggle(self, **kwargs):\n",
    "        \"\"\"Routine for when a new USFS polygon is selected.\"\"\"\n",
    "        if list(kwargs.keys()) != ['event', 'properties']: # check event\n",
    "            return(None)                                   # skip basemap\n",
    "        self.on = False if self.on else True               # update status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prog = lambda m: dict(min=0, max=m, value=0)\n",
    "xrds = lambda l: xr.concat([s.xr for s in l], \"sample\")\n",
    "\n",
    "\n",
    "def get_on_lyrs(column=None):\n",
    "    \"\"\"\n",
    "    Returns a subset of layers df, only \"on\" layers. If keyword \n",
    "    argument 'column' will return only that column.\n",
    "    \"\"\"\n",
    "    on = [i for i,row in layers.iterrows() if row[\"layer\"].on]\n",
    "    sdf = layers.iloc[on][column] if column else layers.iloc[on]\n",
    "    return(sdf)\n",
    "\n",
    "\n",
    "def submit_handler(b):\n",
    "    \"\"\"Resets UI and sends requests to SMV when new submit.\"\"\"\n",
    "    \n",
    "    lyron = get_on_lyrs()\n",
    "    for i, row in lyron.iterrows():             # loop over samples col\n",
    "        if not row[\"layer\"].dl:                 # if not downloaded yet\n",
    "            samp = row.samples[\"samp\"].tolist() # get samples\n",
    "            for a,v in prog(len(samp)).items(): # reset progress bar\n",
    "                setattr(progress, a, v)\n",
    "            for s in samp:                      # loop over sample pts\n",
    "                progress.value += 1             # update progress bar\n",
    "                s.update(**pt_status_on)        # update style\n",
    "                s.submit()                      # download the data\n",
    "            layers.at[i,\"xr\"] = xrds(samp)      # make xr dataset\n",
    "            row[\"layer\"].dl = True              # set dl status to True\n",
    "    submit.disabled = True                      # disable submit button\n",
    "\n",
    "submit.on_click(submit_handler)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def layer_click_handler(**kwargs): \n",
    "    \"\"\"\n",
    "    Routine for when a new USFS polygon is selected. Layer.toggle\n",
    "    internal updater should evaluate first.\n",
    "    \"\"\"\n",
    "    if list(kwargs.keys()) != ['event', 'properties']: # check event\n",
    "         return(None)                         # skip basemap\n",
    "    i = int(kwargs[\"properties\"][\"id\"])       # set selected poly id\n",
    "    l = layers.iloc[i]                        # get row for selected\n",
    "    lo = l.layer                              # get Layer class inst\n",
    "    pteval = points.add_layer if lo.on else points.remove_layer\n",
    "    pteval(l[\"points\"])                       # update layer status;\n",
    "    submit.disabled = True if len(get_on_lyrs())==0 else False\n",
    "    if lo.on:\n",
    "        mapw.center, mapw.zoom = (lo.lat,lo.lon), 9 # ctr,zoom map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make all layers and add to map widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_header = [\"id\",\"lat\",\"lon\",\"samp\"]\n",
    "layer_header = [\"id\",\"lat\",\"lon\",\"layer\",\"samples\",\"points\",\"xr\"]\n",
    "\n",
    "layers = []                                   # a temporary list \n",
    "for i, feat in enumerate(features):           # loops over USFS poly feats\n",
    "    \n",
    "    poly = Layer(i, feat, cols[i])            # get Layer class\n",
    "    poly.layer.on_click(layer_click_handler)  # set global callback\n",
    "    polys.add_layer(poly.layer)               # add to polys layer group\n",
    "\n",
    "    pts, samps = LayerGroup(), []             # points layer group; Samples\n",
    "    for j, p in enumerate(poly.ease):         # loop over EASE grid pts\n",
    "        s = Sample(j, p[0], p[1])             # make a Sample instance\n",
    "        pts.add_layer(s.pt)                   # add to points layer group\n",
    "        samps.append((j, p[0], p[1], s))      # append tuple to the list  \n",
    "        \n",
    "    samps = pd.DataFrame(samps, columns=sample_header)       # samples df\n",
    "    layers.append((i,poly.lat,poly.lon,poly,samps,pts,None)) # append\n",
    "    \n",
    "layers = pd.DataFrame(layers, columns=layer_header)          # layers df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ui"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
