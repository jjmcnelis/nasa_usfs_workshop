{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soil Moisture Visualizer by ORNL DAAC\n",
    "\n",
    "**Authenticate with Earthdata Login**               \n",
    "A NASA Earthdata account is required to access data through the Soil Moisture Visualizer (SMV). Running the cell below will prompt you for your username and password. Go to the link to register an account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ursjupyter import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "Two feature layers from the US Forest Service:\n",
    "* *Study areas* (what do we call these Yaxing?)\n",
    "* [Forest Service Regional Boundaries Feature Layer](https://enterprisecontent-usfs.opendata.arcgis.com/datasets/d5f88e90e2dd4e86b65ab51f8fe174a6_1)\n",
    "\n",
    "Both were converted to GeoJSON with the GDAL/OGR [ogr2ogr](https://www.gdal.org/ogr2ogr.html) binary utility by OSGeo:\n",
    "```\n",
    "ogr2ogr -f geojson <output>.json <input>.shp\n",
    "```\n",
    "\n",
    "**SMV datasets are explained later.**\n",
    "\n",
    "## Choose a USFS site\n",
    "Click a USFS study site to submit a series of download requests to the SMV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45db65e69ed24b59a567e51d2e861379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='\\n    <h3>Soil Moisture Visualizer</h3>\\n    <p>Do such and such.</br>1. </br>a. </…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from smvjupyter import *                                       # import the UI\n",
    "\n",
    "usfs_sites = \"docs/usfs_sites/Sites_lf_geo.json\"               # USFS sites\n",
    "usfs_regions = \"docs/usfs_admin/USFS_Regional_Boundaries.json\" # admin regions\n",
    "\n",
    "app = JupyterSMV(usfs_sites, anc=usfs_regions)           # init UI\n",
    "app.ui                                                         # display UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soil Moisture Visualizer (SMV) datasets\n",
    "\n",
    "### Soil Moisture Active Passive (SMAP)\n",
    "Four SMAP datasets are available through the Soil Moisture Visualizer (SMV):\n",
    "* Root zone (0-100cm) volumetric soil moisture (m3/m3)\n",
    "* Surface zone (0-5cm) volumetric soil moisture (m3/m3)\n",
    "* Gross Primary Productivity (g m-2 d-1)\n",
    "* Net Ecosystem Exchange (g m-2 d-1)\n",
    "\n",
    "This plot shows the time series of SMAP datasets averaged over the 58 SMV sample locations covering the area of USFS site 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3630a94df7b4b6da2875c0ffe15d10e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Map(basemap={'url': 'https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png', 'max_zoom': 19, 'attr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "selected_layer = app.layers.iloc[app.selected]\n",
    "xrdataset = selected_layer.xr\n",
    "\n",
    "mapw1, output1 = poly_mapper(selected_layer)                                 # get some widgets\n",
    "display(HBox([mapw1, output1]))\n",
    "\n",
    "plotters = [(xrdataset[\"SMAP_surface\"], 0, None, None),                      # a list of lines to plot and\n",
    "            (xrdataset[\"SMAP_rootzone\"], 0, None, \"m3/m3\"),                  #   their respective properties:\n",
    "            (xrdataset[\"GPP_mean\"], 1, \"green\", None),                       #   dataset, axis, color\n",
    "            (xrdataset[\"NEE_mean\"], 1, \"purple\", \"g m-2 d-1\")]\n",
    "\n",
    "legend_args = {\"loc\": 0,\"framealpha\": 1}\n",
    "\n",
    "with output1:\n",
    "    fig, axs = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(15, 8))  # initialize the figure\n",
    "    for p in plotters:                                                       # loop over list above\n",
    "        data, axis_num, line_color, y_label = p                              # dataset, axis, color\n",
    "        line_args = {\"x\": \"time\", \"add_legend\": False,                       # a dictionary of line properties\n",
    "                    \"ax\": axs[axis_num], \n",
    "                    \"color\": line_color}                                     \n",
    "        data = data.sel(stat=\"Mean\")\n",
    "        tmpmean, tmpstd = data.mean(\"sample\"), data.std(\"sample\")            # get mean,std of sample dimension\n",
    "        tmpmean.plot.line(label=data.attrs[\"description\"], **line_args)      # plot a line\n",
    "        axs[axis_num].set_title(None)                                        # axis title\n",
    "        axs[axis_num].set_ylabel(y_label)                                    # plot axis labels\n",
    "    \n",
    "    axs[0].legend(loc=0, framealpha=1); axs[1].legend(loc=0, framealpha=1)   # set legend properties\n",
    "    axs[0].set_title(\"SMAP Datasets: USFS site 1\", loc='left')               # plot title\n",
    "    axs[0].set_xlabel(None)                                                  # remove axis 1 xlabel\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In situ, airborne soil moisture observations for all datasets\n",
    "The aggregation and harmonization of the various soil moisture datasets gives flexibility for a wide range of implementation scenarios. The next cell plots the average through time of all in situ and airborne soil moisture datasets available through the SMV. The two sets of data can be selection from the big `xarray` dataset with a few methods:\n",
    "* select datasets with *in situ* and *airborne* for the *type* attribute\n",
    "* select the *Mean* statistic from the *stat* dimension and the desired site from the *site* dimension\n",
    "* average the remaining data over the *sample* dimension\n",
    "\n",
    "Here's how it looks in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f690fb9d9e465381f2b00691204972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(layout=Layout(width='75%')), Map(basemap={'url': 'https://{s}.tile.openstreetmap.org/{z}…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mapw2, output2 = poly_mapper(selected_layer)\n",
    "display(HBox([output2, mapw2]))\n",
    "\n",
    "insitu = xrdataset.filter_by_attrs(type=\"in situ\")              # select in situ, and\n",
    "airborne = xrdataset.filter_by_attrs(type=\"airborne\")           # airborne datasets\n",
    "\n",
    "with output2:\n",
    "    fig, axs = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(15, 8))\n",
    "    plotters = [(insitu, axs[0], \"In situ soil moisture\"), \n",
    "                (airborne, axs[1], \"Airborne soil moisture\")]\n",
    "    \n",
    "    for p in plotters:\n",
    "        data, ax, title = p\n",
    "        data = data.sel(stat=\"Mean\").mean(dim=\"sample\")         # select stat \"Mean\" and site N. avg sample dim\n",
    "        tmpstack = np.stack([data[v].data for v in data])       # collapse dataset into a stacked array\n",
    "        tmpmean = np.nanmean(tmpstack, axis=0)                  # calculate mean over time axis (0)\n",
    "        tmpstd = np.nanstd(tmpstack, axis=0)                    # calculate mean over time axis (0)\n",
    "        tmptime = data.time.data\n",
    "        \n",
    "        ax.plot_date(x=tmptime, y=tmpmean, color=\"black\", linestyle=\"solid\", marker=None)\n",
    "        ax.fill_between(tmptime, (tmpmean-tmpstd), (tmpmean+tmpstd), color=\"gray\", alpha=0.2)\n",
    "        ax.set_ylim(0, 30)\n",
    "        ax.set_title(title+\" [avg of available SMV datasets]\")\n",
    "        ax.set_ylabel(\"m3/m3\")\n",
    "        ax.grid('on', alpha=0.25)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f3cdc0bedb448c8bee61c06678b043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(layout=Layout(width='75%')), Map(basemap={'url': 'https://{s}.tile.openstreetmap.org/{z}…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mapw3, output3 = poly_mapper(selected_layer)\n",
    "display(HBox([output3, mapw3]))\n",
    "\n",
    "insitu = xrdataset.filter_by_attrs(type=\"in situ\")              # select in situ, and\n",
    "airborne = xrdataset.filter_by_attrs(type=\"airborne\")           # airborne datasets\n",
    "spaceborne = xrdataset.filter_by_attrs(type=\"spaceborne\")           # airborne datasets\n",
    "\n",
    "\n",
    "with output3:\n",
    "    fig, axs = plt.subplots(nrows=3, ncols=1, sharex=True, figsize=(15, 8))\n",
    "    plotters = [(insitu, axs[0], \"In situ soil moisture\"), \n",
    "                (airborne, axs[1], \"Airborne soil moisture\"),\n",
    "                (spaceborne, axs[2], \"Spaceborne soil moisture\")]\n",
    "    \n",
    "    for p in plotters:\n",
    "        data, ax, title = p\n",
    "        data = data.sel(stat=\"Mean\").mean(dim=\"sample\")         # select stat \"Mean\" and site N. avg sample dim\n",
    "        tmpstack = np.stack([data[v].data for v in data])       # collapse dataset into a stacked array\n",
    "        tmpmean = np.nanmean(tmpstack, axis=0)                  # calculate mean over time axis (0)\n",
    "        tmpstd = np.nanstd(tmpstack, axis=0)                    # calculate mean over time axis (0)\n",
    "        tmptime = data.time.data\n",
    "        \n",
    "        ax.plot_date(x=tmptime, y=tmpmean, color=\"black\", linestyle=\"solid\", marker=None)\n",
    "        ax.fill_between(tmptime, (tmpmean-tmpstd), (tmpmean+tmpstd), color=\"gray\", alpha=0.2)\n",
    "        ax.set_ylim(0, 30)\n",
    "        ax.set_title(title+\" [avg of available SMV datasets]\")\n",
    "        ax.set_ylabel(\"m3/m3\")\n",
    "        ax.grid('on', alpha=0.25)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'CRN_rootzone' (time: 204)>\n",
       "array([nan, nan, nan, ..., nan, nan, nan])\n",
       "Coordinates:\n",
       "  * time     (time) datetime64[ns] 2002-04-30 2002-05-31 ... 2019-03-31"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monthly = insitu.sel(stat=\"Mean\", drop=True).mean(\"sample\").resample(time=\"1m\").mean() #.sel(time=\"time.month\")\n",
    "monthly2 = monthly.groupby(\"time.year\").assign_coords().set_index(year=\"year\")\n",
    "monthly2 = monthly2.groupby(\"time.month\").assign_coords().set_index(month=\"month\")\n",
    "monthly2 = monthly2.stack(my=(\"year\",\"month\"))\n",
    "monthly2.isel(my=[1,2,3,4,5])[\"CRN_rootzone\"]#.plot(x=\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AirMOSS datasets available through Soil Moisture Visualizer\n",
    "Five AirMOSS datasets are available through the SMV:\n",
    "* Mean vol (%) soil moisture at 0-5 cm (from in-ground sensors)\n",
    "* Mean vol (%) soil moisture at 0-100 cm (from in-ground sensors)\n",
    "* Mean vol (%) soil moisture at 0 cm (from L2 airborne SAR)\n",
    "* Mean vol (%) soil moisture at 0-30cm (from L2 airborne SAR)\n",
    "* Mean vol (%) soil moisture at 0-100cm\t(from L2 airborne SAR)\n",
    "\n",
    "This plot shows the time series of AirMOSS datasets averaged over the **N** SMV sample locations covering the area of USFS site 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airborne_AirMOSS = xr_complete.filter_by_attrs(source=\"AirMOSS\", type=\"airborne\").sel(stat=\"Mean\", site=9)\n",
    "\n",
    "np.nanmean(insitu_AirMOSS[\"AirMOSS_in-ground_surface\"].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "933635a5cd284ce8977c28e71a5caf62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Map(basemap={'url': 'https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png', 'max_zoom': 19, 'attr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function install_repl_displayhook.<locals>.post_execute at 0x000002315CF3AEA0> (for post_execute):\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "view limit minimum -0.001 is less than 1 and is an invalid Matplotlib date value. This often happens if you pass a non-datetime value to an axis that has datetime units",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Python\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mpost_execute\u001b[1;34m()\u001b[0m\n\u001b[0;32m    147\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mpost_execute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_interactive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m                     \u001b[0mdraw_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[1;31m# IPython >= 2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3\\lib\\site-packages\\matplotlib\\_pylab_helpers.py\u001b[0m in \u001b[0;36mdraw_all\u001b[1;34m(cls, force)\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mf_mgr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_all_fig_managers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mforce\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mf_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstale\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m                 \u001b[0mf_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[0matexit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGcf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdestroy_all\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3\\lib\\site-packages\\matplotlib\\backend_bases.py\u001b[0m in \u001b[0;36mdraw_idle\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2053\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_idle_drawing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2054\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_idle_draw_cntx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2055\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2056\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2057\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdraw_cursor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    435\u001b[0m             \u001b[1;31m# if toolbar:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m             \u001b[1;31m#     toolbar.set_cursor(cursors.WAIT)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 437\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    438\u001b[0m             \u001b[1;31m# A GUI class may be need to update a window using this draw, so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m             \u001b[1;31m# don't forget to call the superclass.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[1;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m     53\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0martist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3\\lib\\site-packages\\matplotlib\\figure.py\u001b[0m in \u001b[0;36mdraw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   1491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1492\u001b[0m             mimage._draw_list_compositing_images(\n\u001b[1;32m-> 1493\u001b[1;33m                 renderer, self, artists, self.suppressComposite)\n\u001b[0m\u001b[0;32m   1494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1495\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'figure'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[1;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[1;31m# Composite any adjacent images together\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[1;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m     53\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0martist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36mdraw\u001b[1;34m(self, renderer, inframe)\u001b[0m\n\u001b[0;32m   2633\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2634\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2635\u001b[1;33m         \u001b[0mmimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_draw_list_compositing_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0martists\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2637\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'axes'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[1;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[1;31m# Composite any adjacent images together\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[1;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m     53\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0martist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36mdraw\u001b[1;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1190\u001b[1;33m         \u001b[0mticks_to_draw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_ticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         ticklabelBoxes, ticklabelBoxes2 = self._get_tick_bboxes(ticks_to_draw,\n\u001b[0;32m   1192\u001b[0m                                                                 renderer)\n",
      "\u001b[1;32mC:\\Python\\Anaconda3\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36m_update_ticks\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   1026\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m         \u001b[0minterval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_view_interval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1028\u001b[1;33m         \u001b[0mtick_tups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter_ticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# iter_ticks calls the locator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1029\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_smart_bounds\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtick_tups\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m             \u001b[1;31m# handle inverted limits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36miter_ticks\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    969\u001b[0m         \u001b[0mIterate\u001b[0m \u001b[0mthrough\u001b[0m \u001b[0mall\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmajor\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mminor\u001b[0m \u001b[0mticks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    970\u001b[0m         \"\"\"\n\u001b[1;32m--> 971\u001b[1;33m         \u001b[0mmajorLocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmajor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    972\u001b[0m         \u001b[0mmajorTicks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_major_ticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmajorLocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    973\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmajor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_locs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmajorLocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3\\lib\\site-packages\\matplotlib\\dates.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1248\u001b[0m         \u001b[1;34m'Return the locations of the ticks'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1249\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1250\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_locator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3\\lib\\site-packages\\matplotlib\\dates.py\u001b[0m in \u001b[0;36mrefresh\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1267\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1268\u001b[0m         \u001b[1;34m'Refresh internal information based on current limits.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1269\u001b[1;33m         \u001b[0mdmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdmax\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewlim_to_dt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1270\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_locator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_locator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3\\lib\\site-packages\\matplotlib\\dates.py\u001b[0m in \u001b[0;36mviewlim_to_dt\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1024\u001b[0m                              \u001b[1;34m'often happens if you pass a non-datetime '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1025\u001b[0m                              \u001b[1;34m'value to an axis that has datetime units'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1026\u001b[1;33m                              .format(vmin))\n\u001b[0m\u001b[0;32m   1027\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnum2date\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum2date\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvmax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: view limit minimum -0.001 is less than 1 and is an invalid Matplotlib date value. This often happens if you pass a non-datetime value to an axis that has datetime units"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "view limit minimum -0.001 is less than 1 and is an invalid Matplotlib date value. This often happens if you pass a non-datetime value to an axis that has datetime units",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Python\\Anaconda3\\lib\\site-packages\\ipykernel\\pylab\\backend_inline.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(close, block)\u001b[0m\n\u001b[0;32m     37\u001b[0m             display(\n\u001b[0;32m     38\u001b[0m                 \u001b[0mfigure_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m                 \u001b[0mmetadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_fetch_figure_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigure_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m             )\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3\\lib\\site-packages\\ipykernel\\pylab\\backend_inline.py\u001b[0m in \u001b[0;36m_fetch_figure_metadata\u001b[1;34m(fig)\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[1;31m# the background is transparent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         ticksLight = _is_light([label.get_color()\n\u001b[1;32m--> 177\u001b[1;33m                                 \u001b[1;32mfor\u001b[0m \u001b[0maxes\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m                                 \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myaxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m                                 for label in axis.get_ticklabels()])\n",
      "\u001b[1;32mC:\\Python\\Anaconda3\\lib\\site-packages\\ipykernel\\pylab\\backend_inline.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    177\u001b[0m                                 \u001b[1;32mfor\u001b[0m \u001b[0maxes\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m                                 \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myaxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m                                 for label in axis.get_ticklabels()])\n\u001b[0m\u001b[0;32m    180\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mticksLight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mticksLight\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mticksLight\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m             \u001b[1;31m# there are one or more tick labels, all with the same lightness\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36mget_ticklabels\u001b[1;34m(self, minor, which)\u001b[0m\n\u001b[0;32m   1289\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mminor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1290\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_minorticklabels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1291\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_majorticklabels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1293\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_majorticklines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36mget_majorticklabels\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1243\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_majorticklabels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1244\u001b[0m         \u001b[1;34m'Return a list of Text instances for the major ticklabels'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1245\u001b[1;33m         \u001b[0mticks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_major_ticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1246\u001b[0m         \u001b[0mlabels1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtick\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtick\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mticks\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtick\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel1On\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1247\u001b[0m         \u001b[0mlabels2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtick\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtick\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mticks\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtick\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel2On\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36mget_major_ticks\u001b[1;34m(self, numticks)\u001b[0m\n\u001b[0;32m   1394\u001b[0m         \u001b[1;34m'get the tick instances; grow as necessary'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1395\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnumticks\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1396\u001b[1;33m             \u001b[0mnumticks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_major_locator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1398\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmajorTicks\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mnumticks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3\\lib\\site-packages\\matplotlib\\dates.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1248\u001b[0m         \u001b[1;34m'Return the locations of the ticks'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1249\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1250\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_locator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3\\lib\\site-packages\\matplotlib\\dates.py\u001b[0m in \u001b[0;36mrefresh\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1267\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1268\u001b[0m         \u001b[1;34m'Refresh internal information based on current limits.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1269\u001b[1;33m         \u001b[0mdmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdmax\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewlim_to_dt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1270\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_locator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_locator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3\\lib\\site-packages\\matplotlib\\dates.py\u001b[0m in \u001b[0;36mviewlim_to_dt\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1024\u001b[0m                              \u001b[1;34m'often happens if you pass a non-datetime '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1025\u001b[0m                              \u001b[1;34m'value to an axis that has datetime units'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1026\u001b[1;33m                              .format(vmin))\n\u001b[0m\u001b[0;32m   1027\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnum2date\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum2date\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvmax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: view limit minimum -0.001 is less than 1 and is an invalid Matplotlib date value. This often happens if you pass a non-datetime value to an axis that has datetime units"
     ]
    }
   ],
   "source": [
    "mapw5,output5 = poly_mapper(selected_layer)\n",
    "display(HBox([mapw5, output5]))\n",
    "\n",
    "AirMOSS = selected_layer.xr.filter_by_attrs(source=\"AirMOSS\", type=\"airborne\").sel(stat=\"Mean\")\n",
    "\n",
    "with output5:\n",
    "    fig, axs = plt.subplots(nrows=3, ncols=1, sharex=True, figsize=(15, 8))\n",
    "    plotters = [(AirMOSS[\"AirMOSS_L2_3_surface\"], 0, \"orange\"),\n",
    "                (AirMOSS[\"AirMOSS_L2_3_rootzone\"], 1, \"purple\"),\n",
    "                (AirMOSS[\"AirMOSS_L4_rootzone\"], 2, \"green\")]\n",
    "\n",
    "    for p in plotters:\n",
    "        data, line_axis, line_color = p\n",
    "        plotargs = {\"x\": \"time\", \"ax\": axs[line_axis], \"color\": line_color}\n",
    "        tmpmean, tmpstd = data.mean(\"sample\"), data.std(\"sample\")\n",
    "        tmpmean.plot.line(label=data.attrs[\"description\"], **plotargs)\n",
    "        (tmpmean-tmpstd).plot.line(ls=\":\", alpha=0.4, **plotargs)\n",
    "        (tmpmean+tmpstd).plot.line(ls=\":\", alpha=0.4, **plotargs)\n",
    "        axs[line_axis].set_xlabel(\"\")\n",
    "        axs[line_axis].set_ylabel(\"m3/m3\")\n",
    "        axs[line_axis].set_title(\"AirMOSS \"+data.attrs[\"description\"])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detailed walkthrough\n",
    "\n",
    "This is an introduction...\n",
    "\n",
    "## Requirements\n",
    "\n",
    "### Python\n",
    "Written and tested in Python3. Should be compatible with Python2 with minor changes.\n",
    "\n",
    "#### Heavy-lifters:\n",
    "* [numpy](https://www.numpy.org/) --- fundamental Python module for processing n-dimensional arrays and for scientific computing in general\n",
    "* [pandas](https://pandas.pydata.org/) --- tabular data structures and analysis tools built on numpy\n",
    "* [xarray](http://xarray.pydata.org/en/stable/) --- higher-level structures built on pandas (and to a lesser degree [dask](https://dask.org/)) that support labeled dimensions, coordinates, and attributes for geodata\n",
    "\n",
    "All three above are well-supported and widely-used in Python data/geoscience communities. `numpy` provides the functionality that all array mathematics in Python is built upon; `pandas` provides familiar tabular data structures thatmake working with relational/labeled data easy and intuitive; and `xarray` provides N-dimensional variants of the core pandas data structures that *in my opinion* feel more intuitive for datasets with a spatial component than an equivalent implementation in pure `pandas`, where data would need to be represented in \"long-and-narrow\" form to allow for the same indexing capabilities.\n",
    "\n",
    "#### Important, but you have other options:\n",
    "* [requests](https://2.python-requests.org/en/master/) --- high-level library for sending HTTP requests; alternatives: urllib/urllib2\n",
    "* [shapely](https://shapely.readthedocs.io/en/latest/project.html) --- format-agnostic library based on [GEOS](https://trac.osgeo.org/geos/) for manipulating and analyzing geometric objects\n",
    "* [matplotlib](https://matplotlib.org/) --- 2D plotting; `pandas` and `xarray` have `matplotlib` functionality built-in, so we only need to borrow tools from a couple of submodules\n",
    "\n",
    "`xarray` could also go in this category.\n",
    "\n",
    "#### Just for fun:\n",
    "* [ipywidgets](https://ipywidgets.readthedocs.io/en/stable/index.html) --- interactive HTML widgets for Jupyter Notebooks and the IPython kernel | [GitHub](https://github.com/jupyter-widgets/ipywidgets)\n",
    "* [ipyleaflet](https://ipyleaflet.readthedocs.io/en/latest/) --- a Jupyter/Leaflet bridge enabling interactive maps inside  notebooks | [GitHub](https://github.com/jupyter-widgets/ipyleaflet)\n",
    "\n",
    "After installing with *pip*, you need to enable the jupyter extensions:\n",
    "```\n",
    "pip install ipywidgets\n",
    "pip install ipyleaflet\n",
    "jupyter nbextension enable --py widgetsnbextension\n",
    "```\n",
    "\n",
    "#### Also:\n",
    "* io, json --- both part of the core library; NOTE: `io.StringIO` is `StringIO.StringIO` in Python2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                          # for everything\n",
    "import pandas as pd                         # for almost everything\n",
    "import xarray as xr                         # for next to almost everything\n",
    "\n",
    "import requests                             # for downloading SMV data\n",
    "from shapely.geometry import shape          # for geometry data comparison\n",
    "\n",
    "import json                                 # for converting between JSON and Python dicts\n",
    "from io import StringIO                     # for making \"pseudo-files\" from downloaded text\n",
    "\n",
    "import ipywidgets as wg                     # for widgets\n",
    "import ipyleaflet as mwg                    # for map widgets\n",
    "import matplotlib.pyplot as plt             # for plotting\n",
    "from matplotlib import cm, colors           # for generating color maps\n",
    "\n",
    "from IPython.display import display         # for telling IPython to render some outputs\n",
    "\n",
    "auth = dict(ORNL_DAAC_USER_NUM=str(32863))  # Jack's usernum; REMOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soil Moisture Visualizer\n",
    "\n",
    "The [table (docs/smvdatasets.csv)](docs/smvdatasets.csv) included in this repo is a copy of the datasets table from the [SMV User Guide](https://daac.ornl.gov/soilmoisture/guide.html). Read it into a data frame using [`pandas.read_csv`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) and display the available datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smv_datasets = pd.read_csv(\"docs/smvdatasets.csv\", index_col=\"dataset\", header=0)\n",
    "smv_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets are updated weekly (every Sunday), but some data sets may have a longer latency period.\n",
    "\n",
    "Start by opening an example dataset from the included [text file](docs/daily-smap-ORNL-DAAC-PccIuo.txt). We use `pandas.read_csv` again, skipping to the fourth line and setting the data frame index to the column labeled *time*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"docs/daily-smap-ORNL-DAAC-PccIuo.txt\", header=4, index_col=\"time\")\n",
    "df.index = pd.to_datetime(df.index)   \n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text file is parsed to a data frame with up to three values per column, delimited by semicolons `;`: min, mean, and max. Do some additional data preparation in the following steps:\n",
    "\n",
    "1. Use [`pandas.Series.str.split`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.split.html) to split each column into three columns;\n",
    "2. replace the empty strings with [`numpy.nan`](https://docs.scipy.org/doc/numpy-1.13.0/user/misc.html) using [`pandas.DataFrame.replace`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html); and\n",
    "3. change the data type from string to float with [`pandas.DataFrame.astype`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html). \n",
    "\n",
    "Then, assign column names and print the head again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[\"AirMOSS_L4_rootzone\"].str.split(\";\", n=2, expand=True)       # split pd column to 3\n",
    "data = data.replace('', np.nan)                                         # set '' to nan\n",
    "data = data.astype(float)                                               # set all to float\n",
    "data.columns = [\"AirMOSS_L4_rootzone_\"+s for s in [\"min\",\"mean\",\"max\"]] # set column names\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pandas` repackages some basic plotting functionality from `matplotlib`. You can call `pandas.DataFrame.plot` or `pandas.Series.plot`. Plot L4 AirMOSS rootzone soil moisture from the example dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"AirMOSS_L4_rootzone_mean\"].plot()\n",
    "data[\"AirMOSS_L4_rootzone_min\"].plot()\n",
    "data[\"AirMOSS_L4_rootzone_max\"].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------\n",
    "## Read USFS data from GeoJSON\n",
    "\n",
    "The original dataset was a shapefile, but we reprojected and saved as GeoJSON using *ogr2ogr* from the GDAL/OGR binaries package available at OSGeo. GeoJSON is [a format for encoding a variety of geographic data structures](http://geojson.org/).\n",
    "\n",
    "Let's open the GeoJSON and extract some information about one of the USFS sites. Read to a dictionary with `json.load` and print the first feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sites/Sites_lf_geo.json\", \"r\") as f:\n",
    "    shapes = json.load(f)   # json load parses open json file to dict\n",
    "\n",
    "shapes.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the first three dictionary items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shapes[\"crs\"]) \n",
    "print(shapes[\"name\"])\n",
    "print(shapes[\"type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of geojson features is stored in the *features* element. Print the keys for the first one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = shapes[\"features\"]\n",
    "feat = features[0]\n",
    "\n",
    "feat.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features have two essential pieces, *geometry* and *properties*, and a third item *type* that acknowledges that the item is a feature. The geometry contains two items: *type*, one of seven possible types, and *coordinates*, a list of coordinates that delineate the feature. Here's an example:\n",
    "```\n",
    " \"type\": \"Feature\",\n",
    "   \"geometry\": {\n",
    "       \"type\": \"Polygon\",\n",
    "       \"coordinates\": [\n",
    "           [\n",
    "               [100.0, 0.0],\n",
    "               [101.0, 0.0],\n",
    "               [101.0, 1.0],\n",
    "               [100.0, 1.0],\n",
    "               [100.0, 0.0]\n",
    "           ]\n",
    "       ]\n",
    "   },\n",
    "```\n",
    "A feature's properties are equivalent to what you'd find in the attribute table if you were to open the file in ArcMap. Print them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat[\"properties\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values are aboveground NPP... ? Use list comprehension to collect the values for mean and standard deviation into a dictionary, and make a data frame from the dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pd.DataFrame({\n",
    "    \"mean\": [v for k,v in feat[\"properties\"].items() if \"MEAN\" in k],\n",
    "    \"std\": [v for k,v in feat[\"properties\"].items() if \"STD\" in k]})\n",
    "\n",
    "stats.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't print the feature's geometry, it will take up the whole page. Instead pass it to `Shapely.geometry.shape`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geom = feat[\"geometry\"]         # geometry from feature 0\n",
    "sgeom = shape(geom)             # Shapely.geometry.shape\n",
    "bnds = sgeom.bounds             # bounding box around geometry\n",
    "cent = sgeom.centroid           # centroid for geometry\n",
    "\n",
    "sgeom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Shapely` provides some convenient methods for getting important spatial metadata about the feature like its centroid and bounding coordinates. We'll use these later.\n",
    "\n",
    "For now, we learn how to render the geometry as an [`ipyleaflet`](https://ipyleaflet.readthedocs.io/en/latest/) map layer. Create the map in four steps:\n",
    "1. Get a basemap and make a tile layer to display in the map widget. You have a lot of options, including layers available through NASA GIBS: `basemaps.NASAGIBS.ModisTerraTrueColorCR` | [More information](https://ipyleaflet.readthedocs.io/en/latest/api_reference/tile_layer.html)\n",
    "2. [Make a map layer from the GeoJSON feature](https://ipyleaflet.readthedocs.io/en/latest/api_reference/geo_json.html) that we explored earlier.\n",
    "3. Make an [`ipyleaflet.LayerGroup`](https://ipyleaflet.readthedocs.io/en/latest/api_reference/layer_group.html) to store an array of point layers that we will generate in the next step.\n",
    "4. [Initialize the map widget](https://ipyleaflet.readthedocs.io/en/latest/api_reference/map.html) with a tuple of layers, the map center and zoom level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmap = mwg.basemap_to_tiles(mwg.basemaps.Esri.WorldImagery) # 1. get a basemap\n",
    "poly = mwg.GeoJSON(data=feat)                               # 2. geojson layer\n",
    "points = mwg.LayerGroup()                                   # 3. points group\n",
    "m1 = mwg.Map(                                               # 4. map widget init\n",
    "    layers=(bmap, poly, points,),  # tuple of map layers\n",
    "    center=(cent.y, cent.x),       # map center (from shapely)\n",
    "    zoom=9)                        # zoom level\n",
    "\n",
    "m1                                 # display the map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the EASE Grid\n",
    "\n",
    "Spatial queries to the Soil Moisture Visualizer return data corresponding to 9x9-km cells within [the EASE grid system](https://nsidc.org/data/ease).\n",
    "\n",
    "The next short section of the notebook demonstrates how to select an array coordinates that represent centroids of  EASE grid cells that fall within the boundary of the `shapely` geometry that we created earlier. The coordinates can then be used to submit a series of requests for data from the SMV.\n",
    "      \n",
    "The EASE grid latitudes and longitudes are stored in two binary files:\n",
    "* [docs/EASE2_M09km.lats.3856x1624x1.double](docs/EASE2_M09km.lats.3856x1624x1.double)\n",
    "* [docs/EASE2_M09km.lats.3856x1624x1.double](docs/EASE2_M09km.lats.3856x1624x1.double)\n",
    "\n",
    "Read the two files to arrays with [`numpy.fromfile`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.fromfile.html), flatten them by calling [`numpy.ndarray.flatten`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.flatten.html), and stack them with [`numpy.dstack`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dstack.html) to make one very long 2-d array of latitude, longitude pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lats = np.fromfile(\"docs/EASE2_M09km.lats.3856x1624x1.double\", dtype=np.float64).flatten() \n",
    "lons = np.fromfile(\"docs/EASE2_M09km.lons.3856x1624x1.double\", dtype=np.float64).flatten()\n",
    "crds = np.dstack((lats,lons))[0]\n",
    "\n",
    "print(\"Array shape: \"+str(crds.shape)); crds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce the giant array using the tuple of `bnds` that we took from the `shapely` geometry earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ease = crds[(bnds[1]<lats)&(lats<bnds[3])&(bnds[0]<lons)&(lons<bnds[2])]\n",
    "\n",
    "print(\"The new array shape: \"+str(ease.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, 108 EASE grid points fall within the minimum rectangular envelope of the geometry. That seems reasonable. Make an [`ipyleaflet.CircleMarker`](https://ipyleaflet.readthedocs.io/en/latest/api_reference/circle_marker.html) for each point and display the updated map widget:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in ease:\n",
    "    pt = mwg.CircleMarker(                       # map point\n",
    "        location=(p[0],p[1]),                    # lat,lon tuple\n",
    "        radius=7,                                # in pixels\n",
    "        stroke=False,\n",
    "        fill_opacity=0.6,\n",
    "        fill_color=\"black\")\n",
    "    points.add_layer(pt)\n",
    "    \n",
    "m1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should eliminate points that fall outside of the actual boundary of the polygon. `shapely.geometry.shape` objects provide a method [`object.contains`](https://shapely.readthedocs.io/en/stable/manual.html) that returns a boolean indicating whether or not it contains an input point. The next cell does the following:\n",
    "1. Clear the points that we added to the map in the last cell.\n",
    "2. Make a simple function `get_point` that returns a `shapely.geometry.shape` for an input lat,lon pair.\n",
    "3. Iterate over the remaining EASE grid points and add to the map widget if shapely *contains* returns True.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_point(p):\n",
    "    \"\"\"Takes input lat,lon pair; returns a shapely point geometry.\"\"\"\n",
    "    s = shape({\"coordinates\": [p[1], p[0]], \"type\": \"Point\"})\n",
    "    return(s)\n",
    "\n",
    "\n",
    "points.clear_layers()                   # clear the map points\n",
    "for p in ease:                          # loop over remaining EASE points\n",
    "    \n",
    "    spt = get_point([p[0], p[1]])       # get a shapely point\n",
    "    if sgeom.contains(spt):             # if poly contains point, \n",
    "        \n",
    "        pt = mwg.CircleMarker(          # make circle marker\n",
    "            location=(p[0],p[1]),\n",
    "            radius=7,\n",
    "            stroke=False,\n",
    "            fill_opacity=0.6,\n",
    "            fill_color=\"black\")\n",
    "        \n",
    "        points.add_layer(pt)            # add to map\n",
    "        \n",
    "m1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a function that includes all of the logic above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ease(shapely_geom):\n",
    "    \"\"\"\n",
    "    Takes an input shapely geometry and returns a list of EASE grid \n",
    "    lat,lon pairs that are contained within it.\n",
    "    \"\"\"\n",
    "\n",
    "    bnds = shapely_geom.bounds                # bounding box of input geom\n",
    "    ease = crds[\n",
    "        (bnds[1]<lats) & (lats<bnds[3]) &     # ybnds < lat < ybnds\n",
    "        (bnds[0]<lons) & (lons<bnds[2])]      # xbnds < lon < xbnds\n",
    "\n",
    "    ease_reduced = []\n",
    "    for p in ease:\n",
    "        \n",
    "        shapely_pt = shape({                  # input to shapely.shape is a\n",
    "            \"type\": \"Point\",                  # python dict equivalent of\n",
    "            \"coordinates\": (p[1], p[0])})     # geojson point geometry\n",
    "        \n",
    "        if shapely_geom.contains(shapely_pt): # if point inside poly\n",
    "            ease_reduced.append([p[0], p[1]]) # return lat, lon tuple\n",
    "\n",
    "    return(ease_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download SMV data with `requests`\n",
    "Each request to SMV takes a latitude `&lt` and longitude `&ln`. This request is for (30,-100):       \n",
    "https://daac.ornl.gov/cgi-bin/viz/download.pl?lt=30&ln=-100&d=smap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt, ln = ease[0]                                               # first EASE point\n",
    "url = \"https://daac.ornl.gov/cgi-bin/viz/download.pl?\"         # SMV download url\n",
    "request_url = url+\"lt={lt}&ln={ln}&d=smap\".format(lt=lt,ln=ln) # complete request url\n",
    "\n",
    "print(\"SMV base url:\\t\"+url); print(\"Full URL:\\t\"+request_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use [`requests.get`](https://2.python-requests.org//en/master/user/quickstart/) (*note: use `session` from the Earthdata authentication at the very beginning of the notebook*) to download the data from the URL and print the first ten lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(request_url, cookies=auth)\n",
    "f = StringIO(r.text)\n",
    "\n",
    "print(\"\\n\".join(f.readlines()[0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the same methods that we covered at the beginning to parse the text to a pandas data frame. We will take that one step further in the next section as we prepare to add another dimension to our SMV dataset.\n",
    "\n",
    "## Reformat SMV data as a netCDF-like `xarray.Dataset`\n",
    "\n",
    "Now add one more step to the **SMV download response** > **pandas data frame** > **split columns to expanded data frame** workflow by making an xarray dataset. Start with the two functions below, `txt_to_pd` and `split_pd`. They do all of the SMV data processing that we've covered to this to point: \n",
    "* convert the request response to a text object; \n",
    "* parse the text to a `pandas` data frame; and,\n",
    "* parse the columns of `min;mean;max` into three new columns.\n",
    "\n",
    "Test them on the dataset that we downloaded before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_pd(response_text):\n",
    "    \"\"\"Parses response.text to data frame with date index.\"\"\"\n",
    "    \n",
    "    f = StringIO(response_text)                          # get file from string\n",
    "    df = pd.read_csv(f, header=4, index_col=\"time\")      # read to df\n",
    "    df.index = pd.to_datetime(df.index)                  # convert index to dates\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "\n",
    "def split_pd(col):\n",
    "    \"\"\"Splits pd column by ; and set all values to float, nan.\"\"\"\n",
    "    \n",
    "    df = col.str.split(\";\",n=2,expand=True)              # split col by ;\n",
    "    df = df.replace('', np.nan)                          # set '' to nan\n",
    "    df = df.astype(float)                                # set all to float\n",
    "    df.columns = [\"Min\",\"Mean\",\"Max\"]                    # add column names\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "#df = txt_to_pd(r.text)        <- make request auth work # parse response.text to df\n",
    "# temporary, remove this, auth\n",
    "df = pd.read_csv(\"docs/daily-smap-ORNL-DAAC-hYa36V.txt\", header=4, index_col=\"time\")\n",
    "df.index = pd.to_datetime(df.index)    \n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "dfs = {col: split_pd(df[col]) for col in df.columns}     # loop over cols and split to dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two `xarray` data structures, [`xarray.DataArray`](http://xarray.pydata.org/en/stable/generated/xarray.DataArray.html#xarray.DataArray) and [`xarray.Dataset`](http://xarray.pydata.org/en/stable/generated/xarray.Dataset.html#xarray.Dataset), are so closely tied to the [`pandas.Series`](http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series) and [`pandas.DataFrame`](http://pandas.pydata.org/pandas-docs/stable/reference/frame.html) that both modules provide methods to convert between them. \n",
    "\n",
    "We need to define the dimensions of our data in order to take full advantage of the features of `xarray`. We do that by making *coordinate* variables that allow for practical indexing of the data along said dimensions. For example, this cell demonstrates how to create three [`xarray.DataArrays`](http://xarray.pydata.org/en/stable/generated/xarray.DataArray.html#xarray.DataArray) that represent:\n",
    "1. the sample dimension --- remember the array of EASE grid points? a dimension to describe the spatial relationship between the different sample datasets will make it easier to summarize the data going forward.\n",
    "2. the latitude dimension, and\n",
    "3. the longitude dimension --- these two are self-explanatory; the latitude and longitude values are arranged along the sample dimension, one pair per sample, to describe the position in space of each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latatts = dict(                                     # CF attributes for latitude\n",
    "    standard_name=\"latitude\",\n",
    "    long_name=\"sample latitude\",\n",
    "    units=\"degrees_north\")\n",
    "\n",
    "lonatts = dict(                                     # CF attributes for longitude\n",
    "    standard_name=\"latitude\",\n",
    "    long_name=\"sample latitude\",\n",
    "    units=\"degrees_north\")\n",
    "\n",
    "s = xr.DataArray(data=[1], dims=[\"sample\"])                                  # 1. \n",
    "latarr = xr.DataArray(data=[lt], coords=[s], dims=[\"sample\"], attrs=latatts) # 2.\n",
    "lonarr = xr.DataArray(data=[ln], coords=[s], dims=[\"sample\"], attrs=lonatts) # 3. \n",
    "\n",
    "latarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting latitude array is very simple, with only one value. \n",
    "\n",
    "The function below converts SMV outputs (as data frames) to an `xarray.DataArray` with three dimensions: the sample dimension (length=1), the time dimension (length=n), and the *stat* dimension, which really only exists because it allows us to keep all of the data in one place. Steps:\n",
    "\n",
    "1. Collect from the SMV datasets table a list of attributes to assign to the array; \n",
    "2. convert the `pandas.DataFrame` to an the `xarray` structure; \n",
    "3. rename the automatically-generated dimension to \"stat\"; and, \n",
    "4. set a new attribute that indicates whether or not the dataset has valid observations; \"allnan\"==0 indicates there are valid data and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd_to_xr(dataset, df):\n",
    "    \"\"\"Makes an xr.Dataset from a pandas column (series) and coords.\"\"\"\n",
    "    \n",
    "    a = smv_datasets.loc[dataset].to_dict()                 # 1.\n",
    "    x = xr.DataArray(df, name=dataset, attrs=a)             # 2.\n",
    "    x = x.rename(dict(dim_1=\"stat\"))                        # 3.\n",
    "    x.attrs[\"allnan\"] = int(np.isnan(np.nanmean(x.data)))   # 4.\n",
    "    \n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over the dictionary of data frames, here's the first one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = {c: pd_to_xr(c,d) for c,d in dfs.items()}\n",
    "xds = xr.merge(ds.values())\n",
    "xds = xds.assign_coords(lat=latarr, lon=lonarr)\n",
    "xdsp = xds[\"\"]\n",
    "xds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is what a single SMV dataset looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xds[\"SMAP_surface\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases it may be advantageous to reorder the dimensions over which the data are arranged. You can transpose the 2-d array with [`xarray.Dataset.transpose`](http://xarray.pydata.org/en/stable/generated/xarray.Dataset.transpose.html):\n",
    "\n",
    "```\n",
    "xdsT = xds.transpose()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the \"plottable\" datasets\n",
    "Remember that we added an attribute to each SMV dataset that indicates whether or not the mean\\*min\\*max array is entirely nodata: *allnan*\n",
    "\n",
    "Exclude SMV datasets that are entirely nodata using [`xarray.Dataset.filter_by_attrs`](http://xarray.pydata.org/en/stable/generated/xarray.Dataset.filter_by_attrs.html#xarray.Dataset.filter_by_attrs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pds = xds.filter_by_attrs(allnan=0).sel(stat=\"Mean\")\n",
    "pds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pds.filter_by_attrs(units=\"m3/m3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = list(set([pds[d].attrs[\"units\"] for d in pds]))\n",
    "print(units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "allnan = lambda v: np.count_nonzero(~np.isnan(v.data))==0\n",
    "units = [\n",
    "    'g m-2 d-1', \n",
    "    'm3/m3', \n",
    "    'degrees C', \n",
    "    'mm/day']\n",
    "\n",
    "fig, axs = plt.subplots(4, 1, sharex=True, figsize=(15, 12))\n",
    "\n",
    "for i, unit in enumerate(units):\n",
    "    \n",
    "    variables = pds.filter_by_attrs(units=unit)\n",
    "    for v in variables:\n",
    "        if v!=\"COSMOS_surface\":\n",
    "            a = pds[v]\n",
    "            if not allnan(a):              \n",
    "                pds[v].plot(ax=axs[i])\n",
    "                axs[i].set_title(None)\n",
    "                axs[i].set_xlabel(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also filter by any of the other attribute(s) that we assigned from the SMV datasets table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pds.filter_by_attrs(source=\"SMAP\", soil_zone=\"rootzone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slice/filter using dimension-based criteria\n",
    "Filter by the *stat* dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pds.sel(stat=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or the *time* dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = pds.time.data\n",
    "print(time[10]); print(time[20])\n",
    "\n",
    "pds.sel(time=slice(time[10],time[20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature becomes more useful as you add more dimensions to your dataset. We'll use it to filter across three dimensions once we add more sample locations to this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pds[\"SMAP_surface\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a simple interactive plotting UI\n",
    "We use the logic above to drive the plotting UI. We can get a list of the attributes to filter by using list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = list(set([pds[d].attrs[\"source\"] for d in pds]))\n",
    "stype = list(set([pds[d].attrs[\"type\"] for d in pds]))\n",
    "soil_zone = list(set([pds[d].attrs[\"soil_zone\"] for d in pds]))\n",
    "\n",
    "print(source); print(stype); print(soil_zone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRACE has very few observations, so we don't really need the source filter. And all data are from spaceborne datasets, so the only relevant attribute filter for this dataset is the *soil_zone*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dates = pds.sel(stat=\"mean\").dropna(dim=\"time\", how=\"all\").time.data\n",
    "dates = dates.astype('M8[D]')\n",
    "\n",
    "time_slider = wg.SelectionRangeSlider(\n",
    "    options=dates, \n",
    "    index=(0, len(dates)-1),\n",
    "    continuous_update=False,\n",
    "    layout=wg.Layout(width=\"auto\"))\n",
    "\n",
    "widgets = dict(\n",
    "    Time=time_slider, \n",
    "    By=[\"None\", \"year\", \"month\", \"week\", \"day\"],\n",
    "    Zone=['surface', 'rootzone'],\n",
    "    Mean=True, Min=True, Max=True)\n",
    "\n",
    "# needs to run twice to switch from inline -->\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and display the plot ui:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.rcParams['figure.figsize'] = [12, 5]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "\n",
    "def update(Time, By, Zone, Mean, Min, Max):\n",
    "    \"\"\" \"\"\"\n",
    "\n",
    "    stat = [True]*3 if not any([Mean,Min,Max]) else [Mean,Min,Max]\n",
    "    data = pds.sel(stat=stat)                      # filter by stats\n",
    "    \n",
    "    data = data.filter_by_attrs(soil_zone=Zone)    # filter by attributes\n",
    "\n",
    "    data = data.sel(time=slice(Time[0],Time[1]))   # filter by time;\n",
    "    \n",
    "    xaxis = \"time\" if By == \"None\" else By         # new plot interval\n",
    "    if By is not \"None\":\n",
    "        data = data.sel(stat=\"mean\")\n",
    "        data = data.groupby(\"time.\"+str(By)).mean()\n",
    "\n",
    "    ax.clear()                                     # clear plot\n",
    "    for d in data:                                 # loop over vars\n",
    "        data[d].plot.line(x=xaxis, ax=ax)          # add line\n",
    "    fig.canvas.draw()                              # draw \n",
    "\n",
    "\n",
    "p = wg.interactive(update, **widgets);\n",
    "display(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organize a series of SMV samples into similar structure\n",
    "\n",
    "The capabilities of xarray aren't obvious until you add a second dimension to the data (excluding the unnecessary *stats* dim). You can do everything we just did with pandas. Let's look at our USFS polygon again with a new map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = mwg.LayerGroup()\n",
    "polys = mwg.LayerGroup(layers=(poly,))\n",
    "m2 = mwg.Map(layers=(polys, points, bmap), center=(cent.y, cent.x), zoom=9)\n",
    "\n",
    "m2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Container for point samples\n",
    "`Sample(<id>,<lat>,<lon>)`\n",
    "* id: an integer id unique to the sample within it's input polygon\n",
    "* lat, lon: latitude, longitude numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://daac.ornl.gov/cgi-bin/viz/download.pl?\"\n",
    "\n",
    "class Sample(object):\n",
    "\n",
    "    def __init__(self, i, lat, lon):\n",
    "        \"\"\"Inits with id,lat,lon; makes request string, map point.\"\"\"\n",
    "        self.id, self.lat, self.lon = i, lat, lon               # id, lat, lon\n",
    "        self.rurl = url+\"lt={0}&ln={1}&d=smap\".format(lt,ln)    # request url     \n",
    "        self.pt = mwg.CircleMarker(                             # map point\n",
    "            location=(lat,lon),                                 # lat,lon tuple\n",
    "            radius=7,                                           # in pixels\n",
    "            stroke=False,\n",
    "            fill_opacity=0.6,\n",
    "            fill_color=\"black\")\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        for arg, val in kwargs.items():\n",
    "            setattr(self.pt, arg, val)\n",
    "        \n",
    "    def submit(self):\n",
    "        \"\"\"Called by parent. Downloads url. Updates status.\"\"\"\n",
    "        self.response = requests.get(self.rurl, cookies=auth)   # submit SMV request\n",
    "        self.df = txt_to_pd(self.response.text)                 # read to pandas df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function we made before `get_ease` to get a list of EASE points inside the polygon, make a Sample for each, and organize inside a data frame. Print the first five rows of the data frame and display the updated map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "for i, pt in enumerate(get_ease(sgeom)):\n",
    "    s = Sample(i, pt[0], pt[1])                # make a Sample instance\n",
    "    points.add_layer(s.pt)                     # add map pt to points group\n",
    "    samples.append((i, pt[0], pt[1], s, None)) # append tuple to the list\n",
    "\n",
    "samples = pd.DataFrame(                        # convert list of tuples to df\n",
    "    samples, \n",
    "    columns=[\"id\", \"lat\", \"lon\", \"samp\", \"xr\"])\n",
    "\n",
    "print(samples.head(5)); m2                     # display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a couple more widgets purely for aesthetics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(samples.samp)\n",
    "progress = wg.IntProgress(value=0, min=0, max=n, description=\"Progress: \", layout=wg.Layout(width=\"95%\"))\n",
    "\n",
    "def submit_handler(b):\n",
    "    submit.disabled = True               # disable submit button\n",
    "    for samp in samples.samp:            # loop over sample pts\n",
    "        progress.value += 1              # update progress bar\n",
    "        samp.update(                     # update point style\n",
    "            stroke=True, \n",
    "            color=\"white\", \n",
    "            opacity=0.6)\n",
    "        samp.submit()                    # download the data\n",
    "\n",
    "submit = wg.Button(description='Submit', button_style='success')\n",
    "submit.on_click(submit_handler)\n",
    "\n",
    "wg.VBox([m2,wg.HBox([submit,progress])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully you didn't have any trouble downloading the data. Remember we made the class that binds a map marker to several other items including a pandas data frame that gets created when the sample is retrieved from the SMV.\n",
    "\n",
    "Check the data frame for sample zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples0 = samples.iloc[0]\n",
    "print(samples0); samples0.samp.df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks familiar. Use the steps that we learned before to convert to an xarray dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = xr.DataArray(data=[samples0.id], dims=[\"sample\"])\n",
    "y0 = xr.DataArray(data=[samples0.lat], coords=[s0], dims=[\"sample\"], attrs=latatts)\n",
    "x0 = xr.DataArray(data=[samples0.lon], coords=[s0], dims=[\"sample\"], attrs=lonatts)\n",
    "\n",
    "df0 = samples0.samp.df                                  # get the sample df\n",
    "dfs0 = {col: split_pd(df0[col]) for col in df0.columns} # loop over cols and split to dfs\n",
    "ds0 = {c: pd_to_xr(c,d) for c,d in dfs0.items()}        # make xr datasets for each smv\n",
    "xds0 = xr.merge(ds0.values())                           # merge to one xr dataset\n",
    "xds0 = xds0.assign_coords(lat=y0, lon=x0)               # add coordinate arrays sample \n",
    "xds0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we're at it, wrap all of that up in a function to apply to all of the samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_xr(samp):\n",
    "    \"\"\" \"\"\"\n",
    "    \n",
    "    # get sample, lat, lon xr arrays\n",
    "    s = xr.DataArray(data=[samp.id], dims=[\"sample\"])\n",
    "    y = xr.DataArray(data=[samp.lat], coords=[s], dims=[\"sample\"], attrs=latatts)\n",
    "    x = xr.DataArray(data=[samp.lon], coords=[s], dims=[\"sample\"], attrs=lonatts)\n",
    "\n",
    "    df = samp.df                                         # get the sample df\n",
    "    dfs = {col: split_pd(df[col]) for col in df.columns} # loop over cols and split to dfs\n",
    "    ds = {c: pd_to_xr(c,d) for c,d in dfs.items()}       # make xr datasets for each smv\n",
    "    xds = xr.merge(ds.values())                          # merge to one xr dataset\n",
    "    xds = xds.assign_coords(lat=y, lon=x)                # add coordinate arrays\n",
    "    \n",
    "    return(xds)\n",
    "    \n",
    "\n",
    "for ix, row in samples.iterrows():                       # loop over samples df\n",
    "    samples.at[ix, \"xr\"] = get_sample_xr(row.samp)       # add xr dataset to col\n",
    "    \n",
    "samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check sample ten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.iloc[10].xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, if we organized the data correctly, we can concatenate along the sample dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xds0 = xr.concat(samples.xr.tolist(), \"sample\")\n",
    "xds0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save as a netCDF. The dataset needs another variable and some special attributes to comply with CF (explain CF):\n",
    "* a variable that describes the sequence that makes up the sample dimension\n",
    "* a dataset-level attribute that inidcates the dataset's *featureType*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over sample dim sequence and make strings like: \"sample##\"\n",
    "sample_name_data = [\"sample\"+(\"%02d\" % s) for s in xds0.sample.data]\n",
    "\n",
    "xds0[\"sample_name\"] = xr.DataArray(        # make an xr array\n",
    "    data=sample_name_data, \n",
    "    dims=[\"sample\"], \n",
    "    attrs=dict(                            # cf attributes\n",
    "        long_name=\"sample name\", \n",
    "        cf_role=\"timeseries_id\"))\n",
    "\n",
    "xds0.attrs.update({\n",
    "    \"convention\": \"CF-1.6\", \n",
    "    \"featureType\": \"timeSeries\",\n",
    "    \"source\": \"Soil Moisture Visualizer\",\n",
    "    \"institution\": \"Oak Ridge National Laboratory Distributed Active Archive Center\"})\n",
    "\n",
    "xds0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save with [`xarray.Dataset.to_netcdf`](http://xarray.pydata.org/en/stable/generated/xarray.Dataset.to_netcdf.html#xarray.Dataset.to_netcdf):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xds0.to_netcdf(\"23samples.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nice colors\n",
    "1. use [`numpy.linspace`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linspace.html) to make an array of evenly-spaced values between 0-1 \n",
    "2. map values to **Set3** in [`matplotlib.cm`](https://matplotlib.org/api/cm_api.html) | [colormap reference](https://matplotlib.org/gallery/color/colormap_reference.html)\n",
    "3. convert to hexadecimal with [`matplotlib.colors.to_hex`](https://matplotlib.org/api/_as_gen/matplotlib.colors.to_hex.html#matplotlib.colors.to_hex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cspace = np.linspace(0.0, 1.0, len(features)) # 1\n",
    "rgb = cm.Set3(cspace)                         # 2\n",
    "cols = [colors.to_hex(c[0:3]) for c in rgb]   # 3\n",
    "\n",
    "cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAybe give them the option to import shapefile?\n",
    "```\n",
    "with open(\"sites/Sites_lf_geo.json\", \"r\") as f:\n",
    "    shapes = json.load(f)\n",
    "features = shapes[\"features\"]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "> ### FOR TESTING -->>\n",
    ">Selecting subsets of the data by dimension and attribute:\n",
    ">```\n",
    ">xr_complete = xr.concat(app.layers.xr, dim=\"site\")  # merge all sites\n",
    ">xr_complete.to_netcdf(\"usfs_sites_smv.nc\")          # save to netCDF\n",
    ">xr_complete = xr.open_dataset(\"usfs_sites_smv.nc\")  # open netCDF\n",
    ">sites_SMAP.groupby(\"time.month\").mean(\"sample\")     # group by month\n",
    ">\n",
    "># filter by source, select mean, select site 3\n",
    ">sites_SMAP = xr_complete.filter_by_attrs(source=\"SMAP\").sel(stat=\"Mean\", site=3)\n",
    ">```\n",
    "> ### <<-- FOR TESTING"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
