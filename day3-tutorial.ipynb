{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Soil Moisture at USFS Rangeland Monitoring Sites with Python\n",
    "*Author: ORNL DAAC*  \n",
    "*Date: Apr. 21, 2019*  \n",
    "*Contact for ORNL DAAC: uso@daac.ornl.gov*\n",
    "\n",
    "## Summary\n",
    "Forests in the western U.S. have been more and more affected by drought. For example, there was a severe drought event in the Kaibab National Forest in late 2017. The lack of precipitation on Kaibab National Forest has made 2017-2018 the second driest winter in the past 25 years. It significantly affected the biomass productivity in following years. \n",
    "\n",
    "<img src=\"https://westernnews.media.clients.ellingtoncms.com/img/photos/2018/02/20/Boundary_Fire_t715.jpg\"/>\n",
    "\n",
    "*Image source: https://www.williamsnews.com/news/2018/feb/20/dry-winter-sparks-fire-fears/*\n",
    "\n",
    "In this tutorial, we will dynamically access soil moisture data of multiple sources (i.e. satellite, airborne, and in-situ observations) in the past 10 years from the ORNL DAAC's Soil Moisture Visualizer (https://airmoss.ornl.gov/visualize/) at selected USFS rangeland monitoring sites. We then analyze the time series trends of both surface- and rootzone-level soil moisture at the monitoring sites and compare with the time series trends of the larger USFS Regions. We will also analyze the monthly cycle of soil moisture at those rangeland monitoring sites in years 2017/2018 and compare with the historical mean monthly cycle during 2010-2016.\n",
    "\n",
    "<img src=\"https://www.dropbox.com/s/33dkbz8m6tidzhy/DAAC_logo_blue_text_transparent_bg_web_0.png?dl=1\" width=\"500\"/>\n",
    "\n",
    "The Oak Ridge National Laboratory Distributed Active Archive Center (ORNL DAAC, https://daac.ornl.gov) for biogeochemical dynamics is one of the NASA Earth Observing System Data and Information System (EOSDIS, https://earthdata.nasa.gov/about) data centers managed by the Earth Science Data and Information System (ESDIS) Project, which is responsible for providing scientific and other users access to data from NASA's Earth Science Missions. ORNL DAAC is operated by the ORNL Environmental Sciences Division and is responsible for data archival, product development and distribution, and user support for biogeochemical and ecological data and models.\n",
    "\n",
    "ORNL DAAC archives and publishes datasets of multiple science themes collected and created by a number of NASA science missions, including the Carbon Monitoring System (CMS), Arctic-Boreal Vulnerability Experiment (ABoVE), Global Ecosystem Dynamics Investigation (GEDI) (*L3 & L4 products*), and Earth Venture Suborbital missions (CARVE, AirMOSS, ACT-America, and ATom): https://daac.ornl.gov/get_data/. \n",
    "\n",
    "## Earthdata Account\n",
    "\n",
    "A NASA Earthdata account is required to access data through the Soil Moisture Visualizer (SMV). Running the cell below will prompt you for your username and password. If you do not have a NASA Earthdata, follow the link that is displayed and register before logging in.\n",
    "\n",
    "## Choose a USFS site\n",
    "*Soil Moisture Visualizer datasets are explained later.*\n",
    "\n",
    "Two data layers from the US Forest Service:\n",
    "* Forest Service Rangeland Monitoring Sites\n",
    "* [Forest Service Regional Boundaries](https://enterprisecontent-usfs.opendata.arcgis.com/datasets/d5f88e90e2dd4e86b65ab51f8fe174a6_1)\n",
    "\n",
    "Both data layers were converted from Shapefile to GeoJSON format with the GDAL/OGR [ogr2ogr](https://www.gdal.org/ogr2ogr.html) utility by OSGeo:\n",
    "```\n",
    "ogr2ogr -f geojson <output>.json <input>.shp\n",
    "```\n",
    "\n",
    "Click a USFS study site to submit a series of download requests to the SMV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Login successful. Download with: session.get(url)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de2317a012944d55b496eb084869aafb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(HTML(value='\\n    <h3>Batch download Soil Moisture Visualizer datasets for USFS â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "from ursjupyter import *\n",
    "from smvjupyter import *\n",
    "\n",
    "usfs_sites = \"docs/usfs_sites/Sites_lf_geo.json\"                      # USFS sites\n",
    "usfs_regions = \"docs/usfs_admin/USFS_admin_boundaries.json\"           # USFS administrative regions\n",
    "\n",
    "app = JupyterSMV(usfs_sites, anc=usfs_regions, session=session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the header of the dataset after some formatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot index by location index with a non-integer key",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f96e791c872a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mselected_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselected\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselected_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Mean\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2224\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2225\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2226\u001b[0;31m                 raise TypeError(\"Cannot index by location index with a \"\n\u001b[0m\u001b[1;32m   2227\u001b[0m                                 \"non-integer key\")\n\u001b[1;32m   2228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot index by location index with a non-integer key"
     ]
    }
   ],
   "source": [
    "selected_layer = app.layers.iloc[app.selected]\n",
    "ds = selected_layer.xr.sel(stat=\"Mean\", drop=True)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some plot settings\n",
    "*note: You won't be able to use the interactive plot above after you run this cell. Restart the notebook to enable it again.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rc('font', **{'family': 'normal', 'weight': 'normal', 'size': 14})\n",
    "plt.rc(\"axes\", **{\"xmargin\": 0.01})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary stats for southwest USFS administrative region\n",
    "\n",
    "Pre-calculated using the same workflow as demonstrated towards the bottom of this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usfs_astats = pd.read_csv(\"docs/usfs_admin.csv\", index_col=\"time\")\n",
    "usfs_astats.index = pd.to_datetime(usfs_astats.index)\n",
    "usfs_astats.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(14,7))\n",
    "fig.tight_layout()\n",
    "\n",
    "# loop over some plot parameters\n",
    "for i, p in enumerate([\n",
    "    (0, \"smap_rz\", \"purple\", \"SMAP_rootzone\"), \n",
    "    (0, \"smap_su\", \"darkgreen\", \"SMAP_surface\")]):\n",
    "    \n",
    "    axnum, prefix, clr, name = p              # unpack the iter item\n",
    "    ax = axs[axnum]                           # select axis\n",
    "    \n",
    "    dsmean = usfs_astats[prefix+\"_mean\"]      # get mean of usfs admin region\n",
    "    dsstd = usfs_astats[prefix+\"_std\"]        # ... standarde deviation\n",
    "    stdlo, stdhi = (dsmean-dsstd), (dsmean+dsstd) # get std bounds; plot:\n",
    "    dsmean.plot.line(x=\"time\", ax=ax, color=p[2], label=name, linestyle=\"--\")\n",
    "    ax.fill_between(usfs_astats.index, stdlo, stdhi, color=p[2], alpha=0.2)\n",
    "\n",
    "usfs_stats = selected_layer.layer.stats       # get selected usfs \"site\" stats\n",
    "usfs_deets = selected_layer.layer.details     # ... details\n",
    "usfs_stats.MEAN.plot(ax=axs[1], color=\"black\")# plot mean, plot std hi, lo\n",
    "(usfs_stats.MEAN+usfs_stats.STD).plot(ax=axs[1], color=\"black\", linestyle=\":\")\n",
    "(usfs_stats.MEAN-usfs_stats.STD).plot(ax=axs[1], color=\"black\", linestyle=\":\")\n",
    "\n",
    "# set plot titles, labels, etc -->>\n",
    "axs[1].set_title(\" | \".join([\n",
    "    \"Gross production\", usfs_deets[\"FORESTNAME\"], usfs_deets[\"DISTRICTNA\"]]))\n",
    "axs[0].set_title(\"Volumetric soil moisture (SMAP) | USFS Southwestern region\")\n",
    "axs[0].set_ylabel(\"m3/m3\"); axs[1].set_ylabel(\"pounds/acre\")\n",
    "axs[0].grid(\"on\", alpha=0.25); axs[1].grid(\"on\", alpha=0.25)\n",
    "axs[0].set_xlim(dsmean.first_valid_index(),dsmean.index[-1])\n",
    "axs[0].legend(loc=\"upper left\")\n",
    "axs[0].set_xlabel(None)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volumetric soil moisture by depth zone (surface, rootzone) for each platform type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotters = [\n",
    "    (dict(type=\"in situ\", soil_zone=\"rootzone\"), 0, \"darkorange\"),\n",
    "    (dict(type=\"airborne\", soil_zone=\"rootzone\"), 0, \"darkblue\"),\n",
    "    (dict(type=\"spaceborne\", soil_zone=\"rootzone\"), 0, \"darkgreen\"),\n",
    "    \n",
    "    (dict(type=\"in situ\", soil_zone=\"surface\"), 1, \"darkorange\"),\n",
    "    (dict(type=\"airborne\", soil_zone=\"surface\"), 1, \"darkblue\"),\n",
    "    (dict(type=\"spaceborne\", soil_zone=\"surface\"), 1, \"darkgreen\")\n",
    "]\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(16,8))\n",
    "fig.tight_layout()\n",
    "plt.subplots_adjust(hspace=0.000)\n",
    "\n",
    "for i, p in enumerate(plotters):\n",
    "    \n",
    "    dsweek = ds.filter_by_attrs(**p[0]).resample(time=\"1w\").mean(keep_attrs=True)\n",
    "    dscat = xr.concat(dsweek.values(), \"sample\")\n",
    "    \n",
    "    dsmean = dscat.mean(\"sample\", keep_attrs=True) \n",
    "    dsstd = dscat.std(\"sample\", keep_attrs=True)\n",
    "    stdlo, stdhi = (dsmean-dsstd), (dsmean+dsstd)\n",
    "    \n",
    "    if not all(dsmean.isnull()):\n",
    "        \n",
    "        ax = axs[p[1]]\n",
    "        desc = dsmean.attrs[\"type\"]\n",
    "        zone = dsmean.attrs[\"soil_zone\"]\n",
    "        \n",
    "        dsmean.plot.line(x=\"time\", ax=ax, color=p[2], label=desc, linestyle=\"--\")\n",
    "        ax.fill_between(dsmean.time.data, stdlo, stdhi, color=p[2], alpha=0.2)\n",
    "        ax.set_ylabel(zone+\" (m3/m3)\")\n",
    "\n",
    "\n",
    "axs[0].set_title(\"Vol. soil moisture by platform type (in situ, airborne, spaceborne)\")\n",
    "axs[0].grid(\"on\", alpha=0.2); axs[1].grid(\"on\", alpha=0.2)\n",
    "axs[0].legend(loc=\"upper left\"); axs[1].legend(loc=\"upper left\")\n",
    "axs[0].set_xlabel(None)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize all rootzone soil moisture datasets\n",
    "\n",
    "Concatenate all of the rootzone soil moisture datasets, add some time coordinates to the dataset, and select years 2010 to 2019:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_2012_2019 = ds.sel(time=slice(str(2012),str(2019)))        # select years 2010 to 2019       \n",
    "\n",
    "ds_2012_2019.coords[\"year\"] = ds_2012_2019.time.dt.year       # add year coordinate\n",
    "ds_2012_2019.coords[\"month\"] = ds_2012_2019.time.dt.month     # add month coordinate               \n",
    "ds_2012_2019.coords[\"day\"] = ds_2012_2019.time.dt.dayofyear   # add doy coordinate\n",
    "\n",
    "rootzone = ds_2012_2019.filter_by_attrs(soil_zone=\"rootzone\")\n",
    "rzmean = rootzone.mean(\"sample\", keep_attrs=True)             # calc mean over sample dimension\n",
    "rzcat = xr.concat(rzmean.values(), \"cat\").mean(\"cat\")         # concat all arrays; avg\n",
    "rzcat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group by month and plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 5))                              # init figure\n",
    "ax = plt.gca()                                                 # get figure axes\n",
    "\n",
    "rz_mo = rzcat.groupby(\"month\")                                 # group by month\n",
    "mo_mean, mo_std = rz_mo.mean(), rz_mo.std()                    # calc mean, std\n",
    "mo_stdlow, mo_stdhi = (mo_mean-mo_std), (mo_mean+mo_std)       # get std bounds\n",
    "\n",
    "# plot mean, then standard deviation\n",
    "l1 = mo_mean.plot.line(\n",
    "    ax=ax, x=\"month\", label=\"soil moisture\", linestyle=\"--\", color=\"black\")\n",
    "s1 = ax.fill_between(mo_mean.month, mo_stdlow, mo_stdhi, color=\"gray\", alpha=0.2)\n",
    "\n",
    "# also get SMAP GPP mean by month       \n",
    "gpp = ds_2012_2019[\"GPP_mean\"].mean(\"sample\", keep_attrs=True) # mean sample dim \n",
    "gpp_mo = gpp.groupby(\"month\")                                  # group by day\n",
    "gpp_mean, gpp_std = gpp_mo.mean(), gpp_mo.std()                # get mean, std\n",
    "gpp_stdlow, gpp_stdhi = (gpp_mean-gpp_std), (gpp_mean+gpp_std) # get std bounds\n",
    "\n",
    "# add another y axis to the second plot and plot mean, then standard deviation\n",
    "ax2 = ax.twinx()  \n",
    "l2 = gpp_mean.plot.line(ax=ax2, x=\"month\", label=\"productivity\", color=\"midnightblue\")\n",
    "s2 = ax2.fill_between(\n",
    "    gpp_mean.month, gpp_stdlow, gpp_stdhi, color=\"midnightblue\", alpha=0.2)\n",
    "\n",
    "# plot settings\n",
    "po = l1+l2\n",
    "labels = [o.get_label() for o in po]\n",
    "ax.legend(po, labels, loc=1)\n",
    "ax.grid(\"on\", alpha=0.5)                                        \n",
    "#ax.set_ylim(10, 25); ax2.set_ylim(0, 4)\n",
    "ax.set_title(\"monthly vol. soil moisture by year (0-100cm; all sources)\") \n",
    "ax.set_ylabel(\"Soil moisture (m3/m3)\"); ax2.set_ylabel(\"SMAP GPP (g m-2 d-1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or by day of the year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 5))                               # init figure\n",
    "ax = plt.gca()                                                  # get figure axes\n",
    "\n",
    "rz_day = rzcat.groupby(\"day\")                                   # group by day\n",
    "day_mean, day_std = rz_day.mean(), rz_day.std()                 # get mean, std\n",
    "day_stdlow, day_stdhi = (day_mean-day_std), (day_mean+day_std)  # get std bounds\n",
    "\n",
    "for yr in np.unique(rzcat.year.data):                           # loop over years\n",
    "    ln = rzcat.sel(time=str(yr)).groupby(\"day\").mean()          # average by day \n",
    "    ln.plot.line(x=\"day\", label=str(yr), ax=ax, alpha=0.75)     # plot\n",
    "\n",
    "# plot mean, then standard deviation\n",
    "day_mean.plot.line(ax=ax, x=\"day\", label=\"mean\", linestyle=\"--\", color=\"black\")\n",
    "ax.fill_between(day_mean.day, day_stdlow, day_stdhi, color=\"gray\", alpha=0.2)\n",
    "\n",
    "# some plot settings\n",
    "ax.set_title(\"daily vol. soil moisture by year (0-100cm; all sources)\")\n",
    "ax.legend(loc=1, bbox_to_anchor=(1.15, 1))\n",
    "ax.set_ylabel(\"rootzone (m3/m3)\")\n",
    "ax.set_xlabel(\"day of the year\")\n",
    "ax.grid(\"on\", alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soil Moisture Visualizer (SMV) datasets\n",
    "\n",
    "### Soil Moisture Active Passive (SMAP)\n",
    "Four SMAP datasets are available through the Soil Moisture Visualizer (SMV):\n",
    "* Root zone (0-100cm) volumetric soil moisture (m3/m3)\n",
    "* Surface zone (0-5cm) volumetric soil moisture (m3/m3)\n",
    "* Gross Primary Productivity (g m-2 d-1)\n",
    "* Net Ecosystem Exchange (g m-2 d-1)\n",
    "\n",
    "This plot shows the time series of SMAP datasets averaged over the 58 SMV sample locations covering the area of USFS site 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=3, ncols=1, sharex=True, figsize=(16, 12))\n",
    "fig.tight_layout()\n",
    "plt.subplots_adjust(hspace=0.000)\n",
    "\n",
    "smap = ds.filter_by_attrs(source=\"SMAP\")    # get SMAP datasets\n",
    "\n",
    "for p in [(\"SMAP_surface\", 0, \"darkgreen\"), \n",
    "          (\"SMAP_rootzone\", 1, \"darkgreen\"), \n",
    "          (\"GPP_mean\", 2, None), \n",
    "          (\"NEE_mean\", 2, None)]:\n",
    "    ax = axs[p[1]]\n",
    "    data = smap[p[0]]\n",
    "    desc = data.attrs[\"description\"]\n",
    "    \n",
    "    ptime = data.time.data\n",
    "    pmean = data.mean(\"sample\", keep_attrs=True)\n",
    "    pstd = data.std(\"sample\", keep_attrs=True)\n",
    "    \n",
    "    pmean.plot.line(x=\"time\", ax=ax, label=desc, add_legend=False, color=p[2])\n",
    "    ax.fill_between(ptime, (pmean-pstd), (pmean+pstd), color=\"gray\", alpha=0.2)\n",
    "\n",
    "for i, a in enumerate(axs):\n",
    "    ylab = \"m3/m3\" if i<2 else \"g m-2 d-1\"\n",
    "    a.set_ylabel(ylab)\n",
    "    a.set_xlabel(None)\n",
    "    a.grid('on', alpha=0.25)\n",
    "    a.legend(loc=0, framealpha=1)\n",
    "    \n",
    "axs[0].set_title(\"SMAP: Soil moisture at rootzone (1) and surface (2) depths; GPP and NEE (3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airborne Microwave Observatory of Subcanopy and Subsurface (AirMOSS)\n",
    "\n",
    "Five AirMOSS datasets are available through the SMV:\n",
    "* Mean vol (%) soil moisture at 0-5 cm (from in-ground sensors)\n",
    "* Mean vol (%) soil moisture at 0-100 cm (from in-ground sensors)\n",
    "* Mean vol (%) soil moisture at 0 cm (from L2 airborne SAR)\n",
    "* Mean vol (%) soil moisture at 0-30cm (from L2 airborne SAR)\n",
    "* Mean vol (%) soil moisture at 0-100cm\t(from L2 airborne SAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airmoss = ds.filter_by_attrs(source=\"AirMOSS\")\n",
    "airmoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AirMOSS data are relatively sparse. The only one suitable for plotting a time series is the level rootzone soil moisture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airmoss = ds.filter_by_attrs(source=\"AirMOSS\")\n",
    "\n",
    "if xr.concat(airmoss.isnull().values(), \"null_check\").data.all():\n",
    "    print(\"No AirMOSS observations for this site!\")\n",
    "else:\n",
    "    \n",
    "    data = airmoss[\"AirMOSS_L4_rootzone\"]\n",
    "    ptime = data.time.data\n",
    "    pmean = data.mean(\"sample\", keep_attrs=True)\n",
    "    pstd = data.std(\"sample\", keep_attrs=True)\n",
    "\n",
    "    fig = plt.figure(figsize=(18, 4))\n",
    "    ax = plt.gca()\n",
    "    pmean.plot.line(x=\"time\", ax=ax, label=desc, add_legend=False)\n",
    "    ax.fill_between(ptime, (pmean-pstd), (pmean+pstd), color=\"gray\", alpha=0.2)\n",
    "    ax.set_title(\"AirMOSS_L4_rootzone: \"+data.attrs[\"description\"])\n",
    "    ax.set_ylabel(data.attrs[\"units\"])\n",
    "    ax.grid(\"on\", alpha=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daymet\n",
    "\n",
    "Three Daymet datasets are available through the SMV: daily precipitation, minimum and maximum temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax0 = plt.subplots(figsize=(14,5))\n",
    "fig.tight_layout()\n",
    "ax1 = ax0.twinx()\n",
    "\n",
    "daymet = ds.filter_by_attrs(source=\"Daymet\")\n",
    "prcp = daymet.prcp.resample(time=\"1m\", keep_attrs=True).mean(keep_attrs=True).to_series() # avg precip over 1month\n",
    "tmin = daymet.tmin.resample(time=\"1w\", keep_attrs=True).mean(keep_attrs=True)             # avg tmin over 1week\n",
    "tmax = daymet.tmax.resample(time=\"1w\", keep_attrs=True).mean(keep_attrs=True)             # avg tmax over 1week\n",
    "\n",
    "l1 = tmin.plot(ax=ax0, color=\"darkgreen\", label=\"Min temperature weekly average\")\n",
    "l2 = tmax.plot(ax=ax0, color=\"darkorange\", label=\"Max temperature weekly average\")\n",
    "ax1.bar(prcp.index, prcp, width=25, alpha=0.5, label=\"Daily precipitation monthly average\")\n",
    "\n",
    "ax0.grid(\"on\", alpha=0.5)\n",
    "ax0.set_xlim(\"2011-01-01\", \"2017-12-31\")\n",
    "ax0.set_ylabel(tmin.attrs[\"units\"])\n",
    "ax1.set_ylabel(daymet.prcp.attrs[\"units\"])\n",
    "ax0.legend(loc=2); ax1.legend(loc=1)\n",
    "ax0.set_title(\"Daymet surface meteorology: tmin, tmax, prcp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rootzone and surface soil moisture\n",
    "Soil moisture data are provided for two general depth zones: rootzone and surface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soil_moisture = ds.filter_by_attrs(units=\"m3/m3\")\n",
    "rootzone = soil_moisture.filter_by_attrs(soil_zone=\"rootzone\")\n",
    "surface = soil_moisture.filter_by_attrs(soil_zone=\"surface\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot all of the soil moisture observations and the means for both soil depths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=4, ncols=1, sharex=True, figsize=(16, 12))\n",
    "fig.tight_layout()\n",
    "plt.subplots_adjust(hspace=0.000)\n",
    "\n",
    "for plotter in [(0, \"rootzone\", rootzone), (2, \"surface\", surface)]:\n",
    "    axa, axb = axs[plotter[0]+1], axs[plotter[0]]\n",
    "    \n",
    "    arrays, clrs = [], get_colors(len(plotter[2]), cmap=cm.Dark2)\n",
    "    for i, dataset in enumerate(plotter[2].values()):\n",
    "        name = dataset.name\n",
    "        data = dataset.mean(\"sample\") if \"sample\" in dataset.dims else dataset\n",
    "        if not all(data.isnull()):\n",
    "            data.plot.line(ax=axa, x=\"time\", label=name, color=clrs[i], add_legend=False)\n",
    "            arrays.append(dataset)\n",
    "\n",
    "    stack = xr.concat(arrays, \"mean\").stack(cat=(\"mean\", \"sample\"))\n",
    "    pmean, pstd = stack.mean(\"cat\"), stack.std(\"cat\")\n",
    "    pmean.plot(x=\"time\", ax=axb, color=\"black\", label=plotter[1]+\" mean\")\n",
    "    axb.fill_between(pmean.time.data, (pmean-pstd), (pmean+pstd), color=\"gray\", alpha=0.2)\n",
    "\n",
    "    axa.legend(loc=2); axb.legend(loc=2)\n",
    "    #axa.set_ylim(0, 35); axb.set_ylim(0, 35)\n",
    "    axa.set_xlabel(None); axb.set_xlabel(None)\n",
    "    axa.grid(\"on\", alpha=0.25); axb.grid(\"on\", alpha=0.25)\n",
    "    axb.set_ylabel(plotter[1]); axa.set_ylabel(\"(m3/m3)\")\n",
    "\n",
    "axs[0].set_title(\"All rootzone (1,2) and surface (3,4) soil moisture observations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In situ, airborne soil moisture observations for all datasets\n",
    "The aggregation and harmonization of the various soil moisture datasets gives flexibility for a wide range of implementation scenarios. The next cell plots the average through time of all in situ and airborne soil moisture datasets available through the SMV. The two sets of data can be selection from the big `xarray` dataset with a few steps:\n",
    "* select datasets with *in situ* and *airborne* for the *type* attribute\n",
    "* select the *Mean* statistic from the *stat* dimension\n",
    "* average the remaining data over the *sample* dimension\n",
    "\n",
    "Here's how it looks in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insitu = ds.filter_by_attrs(type=\"in situ\")                # select in situ, and\n",
    "airborne = ds.filter_by_attrs(type=\"airborne\")             # airborne datasets\n",
    "spaceborne = ds.filter_by_attrs(type=\"spaceborne\")         # spaceborne datasets\n",
    "\n",
    "fig, axs = plt.subplots(nrows=3, ncols=1, sharex=True, figsize=(18, 8))\n",
    "fig.tight_layout()\n",
    "plt.subplots_adjust(hspace=0.000)\n",
    "\n",
    "plotters = [(insitu, axs[0], \"In situ\"), \n",
    "            (airborne, axs[1], \"Airborne\"),\n",
    "            (spaceborne, axs[2], \"Spaceborne\")]\n",
    "\n",
    "for p in plotters:\n",
    "    data, ax, title = p\n",
    "    data = data.mean(dim=\"sample\")                          # select stat \"Mean\" and site N. avg sample dim\n",
    "    tmpstack = np.stack([data[v].data for v in data])       # collapse dataset into a stacked array\n",
    "    tmpmean = np.nanmean(tmpstack, axis=0)                  # calculate mean over time axis (0)\n",
    "    tmpstd = np.nanstd(tmpstack, axis=0)                    # calculate mean over time axis (0)\n",
    "    tmptime = data.time.data\n",
    "\n",
    "    ax.plot_date(x=tmptime, y=tmpmean, color=\"black\", linestyle=\"solid\", marker=None)\n",
    "    ax.fill_between(tmptime, (tmpmean-tmpstd), (tmpmean+tmpstd), color=\"gray\", alpha=0.2)\n",
    "    ax.set_ylabel(title)\n",
    "    ax.grid('on', alpha=0.25)\n",
    "\n",
    "axs[0].set_title(\"Time series avg of all available soil moisture data (m3/m3)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detailed walkthrough\n",
    "Steps below explain how we request the SMV data and construct the dataset from a list of sample locations.\n",
    "\n",
    "\n",
    "## Requirements\n",
    "\n",
    "### Python\n",
    "Written and tested in Python3. Should be compatible with Python2 with minor changes.\n",
    "\n",
    "* [numpy](https://www.numpy.org/) --- fundamental Python module for processing n-dimensional arrays and for scientific computing in general\n",
    "* [pandas](https://pandas.pydata.org/) --- tabular data structures and analysis tools built on numpy\n",
    " (and to a lesser degree [dask](https://dask.org/)) that support labeled dimensions, coordinates, and attributes for geodata\n",
    "\n",
    "Fundamental python science libraries. `numpy` for numeric arrays; `pandas` for table structure \n",
    "\n",
    "* [xarray](http://xarray.pydata.org/en/stable/) --- ndimensional labeled arrays, works like pandas\n",
    "* [requests](https://2.python-requests.org/en/master/) --- high-level library for sending HTTP requests; alternatives: urllib/urllib2\n",
    "* [shapely](https://shapely.readthedocs.io/en/latest/project.html) --- python geometric analysis based on [GEOS](https://trac.osgeo.org/geos/)\n",
    "* [matplotlib](https://matplotlib.org/) --- 2D plotting; `pandas` and `xarray` have `matplotlib` built-in; \n",
    "\n",
    "* io, json --- base python; `io.StringIO` is `StringIO.StringIO` in python 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import json\n",
    "from math import ceil\n",
    "from io import StringIO\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from shapely.geometry import shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soil Moisture Visualizer\n",
    "\n",
    "The [table (docs/smvdatasets.csv)](docs/smvdatasets.csv) included in this repo is a copy of the datasets table from the [SMV User Guide](https://daac.ornl.gov/soilmoisture/guide.html). Read it into a data frame using [`pandas.read_csv`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) and display the available datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smv_datasets = pd.read_csv(\"docs/smvdatasets.csv\", index_col=\"dataset\", header=0)\n",
    "smv_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets are updated weekly (every Sunday), but some data sets may have a longer latency period.\n",
    "\n",
    "Start by opening an example dataset from the included [text file](docs/daily-smap-ORNL-DAAC-PccIuo.txt). We use `pandas.read_csv` again, skipping to the fourth line and setting the data frame index to the column labeled *time*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"docs/daily-smap-ORNL-DAAC-hYa36V.txt\", header=4, index_col=\"time\")\n",
    "df.index = pd.to_datetime(df.index)   \n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text file is parsed to a data frame with up to three values per column, delimited by semicolons `;`: min, mean, and max. Do some additional data preparation in the following steps:\n",
    "\n",
    "1. Use [`pandas.Series.str.split`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.split.html) to split each column into three columns;\n",
    "2. replace the empty strings with [`numpy.nan`](https://docs.scipy.org/doc/numpy-1.13.0/user/misc.html) using [`pandas.DataFrame.replace`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html); and\n",
    "3. change the data type from string to float with [`pandas.DataFrame.astype`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html). \n",
    "\n",
    "Then, assign column names and print the head again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[\"AirMOSS_L4_rootzone\"].str.split(\";\", n=2, expand=True)       # split pd column to 3\n",
    "data = data.replace('', np.nan)                                         # set '' to nan\n",
    "data = data.astype(float)                                               # set all to float\n",
    "data.columns = [\"AirMOSS_L4_rootzone_\"+s for s in [\"min\",\"mean\",\"max\"]] # set column names\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pandas` repackages some basic plotting functionality from `matplotlib`. You can call `pandas.DataFrame.plot` or `pandas.Series.plot`. Plot L4 AirMOSS rootzone soil moisture from the example dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"AirMOSS_L4_rootzone_mean\"].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------\n",
    "## Read USFS data from GeoJSON\n",
    "\n",
    "The original dataset was a shapefile, but we reprojected and saved as GeoJSON using *ogr2ogr* from the GDAL/OGR binaries package available at OSGeo. GeoJSON is [a format for encoding a variety of geographic data structures](http://geojson.org/).\n",
    "\n",
    "Let's open the GeoJSON and extract some information about one of the USFS sites. Read to a dictionary with `json.load` and print the first feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"docs/usfs_sites/Sites_lf_geo.json\", \"r\") as f:\n",
    "    shapes = json.load(f) # dict <- json text \n",
    "\n",
    "shapes.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the first three dictionary items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shapes[\"crs\"]) \n",
    "print(shapes[\"name\"])\n",
    "print(shapes[\"type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of geojson features is stored in the *features* element. Print the keys for the first one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = shapes[\"features\"]\n",
    "feat = features[0]\n",
    "\n",
    "feat.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The geometry contains a list of coordinates that make the feature boundary. Here's an example:\n",
    "```\n",
    " \"type\": \"Feature\",\n",
    "   \"geometry\": {\n",
    "       \"type\": \"Polygon\",\n",
    "       \"coordinates\": [\n",
    "           [\n",
    "               [100.0, 0.0],\n",
    "               [101.0, 0.0],\n",
    "               [101.0, 1.0],\n",
    "               [100.0, 1.0],\n",
    "               [100.0, 0.0]\n",
    "           ]\n",
    "       ]\n",
    "   },\n",
    "```\n",
    "A feature's properties are equivalent to what you'd find in the attribute table if you were to open the file in ArcMap. Print them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat[\"properties\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values are aboveground NPP. Collect the values for mean and standard deviation into a dictionary, and make a data frame from the dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pd.DataFrame({\n",
    "    \"mean\": [v for k,v in feat[\"properties\"].items() if \"MEAN\" in k],\n",
    "    \"std\": [v for k,v in feat[\"properties\"].items() if \"STD\" in k]})\n",
    "\n",
    "stats.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a `Shapely.geometry.shape`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geom = feat[\"geometry\"]         # geometry from feature 0\n",
    "sgeom = shape(geom)             # Shapely.geometry.shape\n",
    "bnds = sgeom.bounds             # bounding box around geometry\n",
    "cent = sgeom.centroid           # centroid for geometry\n",
    "\n",
    "sgeom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Shapely` has convenient way to get some spatial metadata about the feature like the centroid and bounding coordinates.\n",
    "\n",
    "* [ipywidgets](https://ipywidgets.readthedocs.io/en/stable/index.html) --- interactive HTML widgets for Jupyter Notebooks and the IPython kernel | [GitHub](https://github.com/jupyter-widgets/ipywidgets)\n",
    "* [ipyleaflet](https://ipyleaflet.readthedocs.io/en/latest/) --- a Jupyter/Leaflet bridge enabling interactive maps inside  notebooks | [GitHub](https://github.com/jupyter-widgets/ipyleaflet)\n",
    "\n",
    "After installing with *pip*, you need to enable the jupyter extensions:\n",
    "```\n",
    "pip install ipywidgets\n",
    "pip install ipyleaflet\n",
    "jupyter nbextension enable --py widgetsnbextension\n",
    "```\n",
    "\n",
    "For now, we learn how to render the geometry as an [`ipyleaflet`](https://ipyleaflet.readthedocs.io/en/latest/) map layer. Create the map in four steps:\n",
    "1. Get a basemap and make a tile layer to display in the map widget. You have a lot of options, including layers available through NASA GIBS: `basemaps.NASAGIBS.ModisTerraTrueColorCR` | [More information](https://ipyleaflet.readthedocs.io/en/latest/api_reference/tile_layer.html)\n",
    "2. [Make a map layer from the GeoJSON feature](https://ipyleaflet.readthedocs.io/en/latest/api_reference/geo_json.html) that we explored earlier.\n",
    "3. Make an [`ipyleaflet.LayerGroup`](https://ipyleaflet.readthedocs.io/en/latest/api_reference/layer_group.html) to store an array of point layers that we will generate in the next step.\n",
    "4. [Initialize the map widget](https://ipyleaflet.readthedocs.io/en/latest/api_reference/map.html) with a tuple of layers, the map center and zoom level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmap = mwg.basemap_to_tiles(mwg.basemaps.Esri.WorldImagery) # 1. get a basemap\n",
    "poly = mwg.GeoJSON(data=feat)                               # 2. geojson layer\n",
    "points = mwg.LayerGroup()                                   # 3. points group\n",
    "m1 = mwg.Map(                                               # 4. map widget init\n",
    "    layers=(bmap, poly, points,),  # tuple of map layers\n",
    "    center=(cent.y, cent.x),       # map center (from shapely)\n",
    "    zoom=9)                        # zoom level\n",
    "\n",
    "m1                                 # display the map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the EASE Grid\n",
    "\n",
    "Soil Moisture Visualizer downloads require a lat/lon to select data from 9x9-km cells within [the EASE grid system](https://nsidc.org/data/ease).\n",
    "\n",
    "The next section shows how to select an array coordinates that represent centroids of EASE grid cells that fall within the boundary of the `shapely` shape above. The coordinates can then be used to submit requests for SMV data.\n",
    "      \n",
    "The EASE grid latitudes and longitudes are stored in two binary files:\n",
    "* [docs/EASE2_M09km.lats.3856x1624x1.double](docs/EASE2_M09km.lats.3856x1624x1.double)\n",
    "* [docs/EASE2_M09km.lats.3856x1624x1.double](docs/EASE2_M09km.lats.3856x1624x1.double)\n",
    "\n",
    "Read them to arrays with [`numpy.fromfile`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.fromfile.html), flatten them by calling [`numpy.ndarray.flatten`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.flatten.html), and stack them with [`numpy.dstack`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dstack.html) to make one very long 2-d array of latitude, longitude pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lats = np.fromfile(\"docs/EASE2_M09km.lats.3856x1624x1.double\", dtype=np.float64).flatten() \n",
    "lons = np.fromfile(\"docs/EASE2_M09km.lons.3856x1624x1.double\", dtype=np.float64).flatten()\n",
    "crds = np.dstack((lats,lons))[0]\n",
    "\n",
    "print(\"Array shape: \"+str(crds.shape)); crds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce the giant array using the tuple of `bnds` that we took from the `shapely` geometry earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ease = crds[(bnds[1]<lats)&(lats<bnds[3])&(bnds[0]<lons)&(lons<bnds[2])]\n",
    "\n",
    "print(\"The new array shape: \"+str(ease.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, 108 EASE grid points fall within the minimum rectangular envelope of the geometry. That seems reasonable. Make an [`ipyleaflet.CircleMarker`](https://ipyleaflet.readthedocs.io/en/latest/api_reference/circle_marker.html) for each point and display the updated map widget:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in ease:\n",
    "    pt = mwg.CircleMarker(                       # map point\n",
    "        location=(p[0],p[1]),                    # lat,lon tuple\n",
    "        radius=7,                                # in pixels\n",
    "        stroke=False,\n",
    "        fill_opacity=0.6,\n",
    "        fill_color=\"black\")\n",
    "    points.add_layer(pt)\n",
    "    \n",
    "m1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now eliminate points that fall outside of the actual boundary of the polygon. `shapely.geometry.shape` has a method [`object.contains`](https://shapely.readthedocs.io/en/stable/manual.html) that returns a boolean indicating whether or not it contains an the input x,y. The next cell does the following:\n",
    "1. Clear the points that we added to the map in the last cell.\n",
    "2. Make a simple function `get_point` that returns a `shapely.geometry.shape` for an input lat,lon pair.\n",
    "3. Iterate over the remaining EASE grid points and add to the map widget if shapely *contains* returns True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_point(p):\n",
    "    \"\"\"Takes input lat,lon pair; returns a shapely point geometry.\"\"\"\n",
    "    s = shape({\"coordinates\": [p[1], p[0]], \"type\": \"Point\"})\n",
    "    return(s)\n",
    "\n",
    "\n",
    "points.clear_layers()                   # clear the map points\n",
    "for p in ease:                          # loop over remaining EASE points\n",
    "    \n",
    "    spt = get_point([p[0], p[1]])       # get a shapely point\n",
    "    if sgeom.contains(spt):             # if poly contains point, \n",
    "        \n",
    "        pt = mwg.CircleMarker(          # make circle marker\n",
    "            location=(p[0],p[1]),\n",
    "            radius=7,\n",
    "            stroke=False,\n",
    "            fill_opacity=0.6,\n",
    "            fill_color=\"black\")\n",
    "        \n",
    "        points.add_layer(pt)            # add to map\n",
    "        \n",
    "m1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a function that includes all of the logic above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ease(shapely_geom):\n",
    "    \"\"\"\n",
    "    Takes an input shapely geometry and returns a list of EASE grid \n",
    "    lat,lon pairs that are contained within it.\n",
    "    \"\"\"\n",
    "\n",
    "    bnds = shapely_geom.bounds                # bounding box of input geom\n",
    "    ease = crds[\n",
    "        (bnds[1]<lats) & (lats<bnds[3]) &     # ybnds < lat < ybnds\n",
    "        (bnds[0]<lons) & (lons<bnds[2])]      # xbnds < lon < xbnds\n",
    "\n",
    "    ease_reduced = []\n",
    "    for p in ease:\n",
    "        \n",
    "        shapely_pt = shape({                  # input to shapely.shape is a\n",
    "            \"type\": \"Point\",                  # python dict equivalent of\n",
    "            \"coordinates\": (p[1], p[0])})     # geojson point geometry\n",
    "        \n",
    "        if shapely_geom.contains(shapely_pt): # if point inside poly\n",
    "            ease_reduced.append([p[0], p[1]]) # return lat, lon tuple\n",
    "\n",
    "    return(ease_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download SMV data with `requests`\n",
    "Each request to SMV takes a latitude `&lt` and longitude `&ln`. This request is for (30,-100):       \n",
    "https://daac.ornl.gov/cgi-bin/viz/download.pl?lt=30&ln=-100&d=smap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt, ln = ease[0]                                               # first EASE point\n",
    "url = \"https://airmoss.ornl.gov/cgi-bin/viz/api/download.pl?\"  # SMV download url\n",
    "request_url = url+\"lt={lt}&ln={ln}&d=smap\".format(lt=lt,ln=ln) # complete request url\n",
    "\n",
    "print(\"base url: \"+url); print(\"full url: \"+request_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use [`requests.get`](https://2.python-requests.org//en/master/user/quickstart/) (*note: use `session` from the Earthdata authentication at the very beginning of the notebook*) to download the data from the URL and print the first ten lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = session.get(request_url)\n",
    "f = StringIO(r.text)\n",
    "\n",
    "print(\"\\n\".join(f.readlines()[0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the same methods that we covered at the beginning to parse the text to a pandas data frame. We will take that one step further in the next section as we prepare to add another dimension to our SMV dataset.\n",
    "\n",
    "## Reformat SMV data as a netCDF-like `xarray.Dataset`\n",
    "\n",
    "Now make an xarray dataset. Start with the two functions below, `txt_to_pd` and `split_pd`. They do all of the SMV data processing that we've covered to this to point: \n",
    "* convert the request response to a text object; \n",
    "* parse the text to a `pandas` data frame; and,\n",
    "* parse the columns of `min;mean;max` into three new columns.\n",
    "\n",
    "Test them on the dataset that we downloaded before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_pd(response_text):\n",
    "    \"\"\"Parses response.text to data frame with date index.\"\"\"\n",
    "    \n",
    "    f = StringIO(response_text)                          # get file from string\n",
    "    df = pd.read_csv(f, header=4, index_col=\"time\")      # read to df\n",
    "    df.index = pd.to_datetime(df.index)                  # convert index to dates\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "\n",
    "def split_pd(col):\n",
    "    \"\"\"Splits pd column by ; and set all values to float, nan.\"\"\"\n",
    "    \n",
    "    df = col.str.split(\";\",n=2,expand=True)              # split col by ;\n",
    "    df = df.replace('', np.nan)                          # set '' to nan\n",
    "    df = df.astype(float)                                # set all to float\n",
    "    df.columns = [\"Min\",\"Mean\",\"Max\"]                    # add column names\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "df = txt_to_pd(r.text)\n",
    "df.index = pd.to_datetime(df.index)\n",
    "dfs = {col: split_pd(df[col]) for col in df.columns}     # loop over cols and split to dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make some *coordinate* variables that simplify indexing the data. This cell demonstrates how to make three [`xarray.DataArrays`](http://xarray.pydata.org/en/stable/generated/xarray.DataArray.html#xarray.DataArray) that represent:\n",
    "1. sample dimension --- lat,lon pairs organized along sample dimension\n",
    "2. latitude dimension, 3. longitude dimension --- one pair per sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latatts = dict(                                     # CF attributes for latitude\n",
    "    standard_name=\"latitude\",\n",
    "    long_name=\"sample latitude\",\n",
    "    units=\"degrees_north\")\n",
    "\n",
    "lonatts = dict(                                     # CF attributes for longitude\n",
    "    standard_name=\"latitude\",\n",
    "    long_name=\"sample latitude\",\n",
    "    units=\"degrees_north\")\n",
    "\n",
    "s = xr.DataArray(data=[1], dims=[\"sample\"])                                  # 1. \n",
    "latarr = xr.DataArray(data=[lt], coords=[s], dims=[\"sample\"], attrs=latatts) # 2.\n",
    "lonarr = xr.DataArray(data=[ln], coords=[s], dims=[\"sample\"], attrs=lonatts) # 3. \n",
    "\n",
    "latarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below converts SMV downloads (as data frames) to an `xarray.DataArray` with three dimensions: the sample dimension (length=1), the time dimension (length=n), and the *stat* dimension, which really only exists because it allows us to keep all of the data in one place. Steps:\n",
    "\n",
    "1. Collect from the SMV datasets table a list of attributes to assign to the array; \n",
    "2. convert the `pandas.DataFrame` to `xarray` \n",
    "3. rename the new dimension to \"stat\"; and, \n",
    "4. set new attribute that indicates whether or not there's valid data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd_to_xr(dataset, df):\n",
    "    \"\"\"Makes an xr.Dataset from a pandas column (series) and coords.\"\"\"\n",
    "    \n",
    "    a = smv_datasets.loc[dataset].to_dict()                 # 1.\n",
    "    x = xr.DataArray(df, name=dataset, attrs=a)             # 2.\n",
    "    x = x.rename(dict(dim_1=\"stat\"))                        # 3.\n",
    "    x.attrs[\"allnan\"] = int(x.isnull().all())               # 4.\n",
    "    \n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over the dictionary of data frames, here's the first one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = {c: pd_to_xr(c,d) for c,d in dfs.items()}\n",
    "xds = xr.merge(ds.values())\n",
    "xds = xds.assign_coords(lat=latarr, lon=lonarr)\n",
    "xds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is what a single SMV dataset looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xds[\"SMAP_surface\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select data by dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pds = xds.sel(stat=\"Mean\", drop=True)\n",
    "pds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick variables by attribute with [`xarray.Dataset.filter_by_attrs`](http://xarray.pydata.org/en/stable/generated/xarray.Dataset.filter_by_attrs.html#xarray.Dataset.filter_by_attrs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pds.filter_by_attrs(units=\"m3/m3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = list(set([pds[d].attrs[\"units\"] for d in pds]))\n",
    "print(units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=3, ncols=1, sharex=True, figsize=(18, 12))\n",
    "fig.tight_layout()\n",
    "plt.subplots_adjust(hspace=0.000)\n",
    "\n",
    "smap = xds.filter_by_attrs(source=\"SMAP\")                                        # get SMAP datasets\n",
    "\n",
    "for p in [(\"SMAP_surface\", 0, \"black\"), (\"SMAP_rootzone\", 1, \"black\"), \n",
    "          (\"GPP_mean\", 2, None), (\"NEE_mean\", 2, None)]:\n",
    "    ax = axs[p[1]]\n",
    "    data = smap[p[0]]\n",
    "    desc = data.attrs[\"description\"]\n",
    "    \n",
    "    data.plot.line(x=\"time\", ax=ax, label=desc, add_legend=False, color=p[2])\n",
    "\n",
    "for i, a in enumerate(axs):\n",
    "    ylab = \"m3/m3\" if i<2 else \"g m-2 d-1\"\n",
    "    a.set_ylabel(ylab)\n",
    "    a.set_xlabel(None)\n",
    "    a.grid('on', alpha=0.25)\n",
    "    a.legend(loc=0, framealpha=1)\n",
    "    \n",
    "axs[0].set_title(\"SMAP: Soil moisture at rootzone (1) and surface (2) depths; GPP and NEE (3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also filter by any of the other attribute(s) that we assigned from the SMV datasets table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pds.filter_by_attrs(source=\"SMAP\", soil_zone=\"rootzone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slice/filter using dimension-based criteria\n",
    "the *time* dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = pds.time.data\n",
    "print(time[10]); print(time[20])\n",
    "\n",
    "pds.sel(time=slice(time[10],time[20]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
